/*! \file mpiroutines.cxx
 *  \brief this file contains routines used with MPI compilation.

    MPI routines generally pertain to domain decomposition or to specific MPI tasks that determine what needs to be broadcast between various threads.

 */

#ifdef USEMPI

//-- For MPI

#include "stf.h"

#ifdef SWIFTINTERFACE
#include "swiftinterface.h"
using namespace Swift;
#endif

/// \name Domain decomposition routines and io routines to place particles correctly in local mpi data space
//@{

/*!
    Determine the domain decomposition.\n
    Here the domains are constructured in data units
    only ThisTask==0 should call this routine. It is tricky to get appropriate load balancing and correct number of particles per processor.\n

    I could use recursive binary splitting like kd-tree along most spread axis till have appropriate number of volumes corresponding
    to number of processors.

    NOTE: assume that cannot store data so position information is read Nsplit times to determine boundaries of subvolumes
    could also randomly subsample system and produce tree from that
    should store for each processor the node structure generated by the domain decomposition
    what I could do is read file twice, one to get extent and other to calculate entropy then decompose
    along some primary axis, then choose orthogonal axis, keep iterating till have appropriate number of subvolumes
    then store the boundaries of the subvolume. this means I don't store data but get at least reasonable domain decomposition

    NOTE: pkdgrav uses orthoganl recursive bisection along with kd-tree, gadget-2 uses peno-hilbert curve to map particles and oct-trees
    the question with either method is guaranteeing load balancing. for ORB achieved by splitting (sub)volume along a dimension (say one with largest spread or max entropy)
    such that either side of the cut has approximately the same number of particles (ie: median splitting). But for both cases, load balancing requires particle information
    so I must load the system then move particles about to ensure load balancing.

    Main thing first is get the dimensional extent of the system.
    then I could get initial splitting just using mid point between boundaries along each dimension.
    once have that initial splitting just load data then start shifting data around.
*/

///determine the initial domains, ie: bisection distance mpi_dxsplit, which is used to determien what processor a particle is assigned to
///here the domains are constructured in data units
void MPIInitialDomainDecomposition(){
    Int_t i,j,k,n,m,temp,count,count2,pc,pc_new, Ntot;
    int Nsplit,isplit;
    Int_t nbins1d,nbins3d, ibin[3];
    Double_t diffsplit;
    int b,a;

    if (ThisTask==0) {
        //first split need not be simply having the dimension but determine
        //number of splits to have Nprocs=a*2^b, where a and b are integers
        //initial integers
        b=floor(log((float)NProcs)/log(2.0))-1;
        a=floor(NProcs/pow(2,b));
        diffsplit=(double)NProcs/(double)a/(double)pow(2,b);
        while (diffsplit!=1) {
            b--;
            a=floor(NProcs/pow(2,b));
            diffsplit=(double)NProcs/(double)a/(double)pow(2,b);
        }
        Nsplit=b+1;
        mpi_ideltax[0]=0;mpi_ideltax[1]=1;mpi_ideltax[2]=2;
        isplit=0;
        for (j=0;j<3;j++) mpi_nxsplit[j]=0;
        for (j=0;j<Nsplit;j++) {
            mpi_nxsplit[mpi_ideltax[isplit++]]++;
            if (isplit==3) isplit=0;
        }
        for (j=0;j<3;j++) mpi_nxsplit[j]=pow(2.0,mpi_nxsplit[j]);
        //and adjust first dimension
        mpi_nxsplit[0]=mpi_nxsplit[0]/2*a;

        //for all the cells along the boundary of axis with the third split axis (smallest variance)
        //set the domain limits to the sims limits
        int ix=mpi_ideltax[0],iy=mpi_ideltax[1],iz=mpi_ideltax[2];
        int mpitasknum;
        for (j=0;j<mpi_nxsplit[iy];j++) {
            for (i=0;i<mpi_nxsplit[ix];i++) {
                mpitasknum=i+j*mpi_nxsplit[ix]+0*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                mpi_domain[mpitasknum].bnd[iz][0]=mpi_xlim[iz][0];
                mpitasknum=i+j*mpi_nxsplit[ix]+(mpi_nxsplit[iz]-1)*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                mpi_domain[mpitasknum].bnd[iz][1]=mpi_xlim[iz][1];
            }
        }
        //here for domains along second axis
        for (k=0;k<mpi_nxsplit[iz];k++) {
            for (i=0;i<mpi_nxsplit[ix];i++) {
                mpitasknum=i+0*mpi_nxsplit[ix]+k*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                mpi_domain[mpitasknum].bnd[iy][0]=mpi_xlim[iy][0];
                mpitasknum=i+(mpi_nxsplit[iy]-1)*mpi_nxsplit[ix]+k*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                mpi_domain[mpitasknum].bnd[iy][1]=mpi_xlim[iy][1];
            }
        }
        //finally along axis with largest variance
        for (k=0;k<mpi_nxsplit[iz];k++) {
            for (j=0;j<mpi_nxsplit[iy];j++) {
                mpitasknum=0+j*mpi_nxsplit[ix]+k*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                mpi_domain[mpitasknum].bnd[ix][0]=mpi_xlim[ix][0];
                mpitasknum=(mpi_nxsplit[ix]-1)+j*mpi_nxsplit[ix]+k*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                mpi_domain[mpitasknum].bnd[ix][1]=mpi_xlim[ix][1];
            }
        }
        //here use the three different histograms to define the boundary
        int start[3],end[3];
        Double_t bndval[3],binsum[3],lastbin;
        start[0]=start[1]=start[2]=0;
        for (i=0;i<mpi_nxsplit[ix];i++) {
            bndval[0]=(mpi_xlim[ix][1]-mpi_xlim[ix][0])*(Double_t)(i+1)/(Double_t)mpi_nxsplit[ix];
            if(i<mpi_nxsplit[ix]-1) {
            for (j=0;j<mpi_nxsplit[iy];j++) {
                for (k=0;k<mpi_nxsplit[iz];k++) {
                    //define upper limit
                    mpitasknum=i+j*mpi_nxsplit[ix]+k*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                    mpi_domain[mpitasknum].bnd[ix][1]=bndval[0];
                    //define lower limit
                    mpitasknum=(i+1)+j*mpi_nxsplit[ix]+k*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                    mpi_domain[mpitasknum].bnd[ix][0]=bndval[0];
                }
            }
            }
            //now for secondary splitting
            if (mpi_nxsplit[iy]>1)
            for (j=0;j<mpi_nxsplit[iy];j++) {
                bndval[1]=(mpi_xlim[iy][1]-mpi_xlim[iy][0])*(Double_t)(j+1)/(Double_t)mpi_nxsplit[iy];
                if(j<mpi_nxsplit[iy]-1) {
                for (k=0;k<mpi_nxsplit[iz];k++) {
                    mpitasknum=i+j*mpi_nxsplit[ix]+k*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                    mpi_domain[mpitasknum].bnd[iy][1]=bndval[1];
                    mpitasknum=i+(j+1)*mpi_nxsplit[ix]+k*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                    mpi_domain[mpitasknum].bnd[iy][0]=bndval[1];
                }
                }
                if (mpi_nxsplit[iz]>1)
                for (k=0;k<mpi_nxsplit[iz];k++) {
                    bndval[2]=(mpi_xlim[iz][1]-mpi_xlim[iz][0])*(Double_t)(k+1)/(Double_t)mpi_nxsplit[iz];
                    if (k<mpi_nxsplit[iz]-1){
                    mpitasknum=i+j*mpi_nxsplit[ix]+k*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                    mpi_domain[mpitasknum].bnd[iz][1]=bndval[2];
                    mpitasknum=i+j*mpi_nxsplit[ix]+(k+1)*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                    mpi_domain[mpitasknum].bnd[iz][0]=bndval[2];
                    }
                }
            }
        }
        cout<<"Initial MPI Domains are: "<<endl;
        for (int j=0;j<NProcs;j++) {
            cout<<"ThisTask= "<<j<<" :: ";
            cout.precision(10);for (int k=0;k<3;k++) cout<<k<<" "<<mpi_domain[j].bnd[k][0]<<" "<<mpi_domain[j].bnd[k][1]<<" | ";cout<<endl;
        }
    }
    //broadcast data
    MPI_Bcast(mpi_domain, NProcs*sizeof(MPI_Domain), MPI_BYTE, 0, MPI_COMM_WORLD);
}


void MPINumInDomain(Options &opt)
{
    //when reading number in domain, use all available threads to read all available files
    //first set number of read threads to either total number of mpi process or files, which ever is smaller
    //store old number of read threads
    if (NProcs==1) {
    Nlocal=Ntotal;
    Nmemlocal=Nlocal;
    return;
    }
    int nsnapread=opt.nsnapread;
    opt.nsnapread=min(NProcs,opt.num_files);
    if(opt.inputtype==IOTIPSY) MPINumInDomainTipsy(opt);
    else if (opt.inputtype==IOGADGET) MPINumInDomainGadget(opt);
    else if (opt.inputtype==IORAMSES) MPINumInDomainRAMSES(opt);
#ifdef USEHDF
    else if (opt.inputtype==IOHDF) MPINumInDomainHDF(opt);
#endif
    opt.nsnapread=nsnapread;
    //adjust the memory allocated to allow some buffer room.
    Nmemlocal=Nlocal*(1.0+opt.mpipartfac);
    if (opt.iBaryonSearch) Nmemlocalbaryon=Nlocalbaryon[0]*(1.0+opt.mpipartfac);

}

void MPIDomainExtent(Options &opt)
{
    if(opt.inputtype==IOTIPSY) MPIDomainExtentTipsy(opt);
    else if (opt.inputtype==IOGADGET) MPIDomainExtentGadget(opt);
    else if (opt.inputtype==IORAMSES) MPIDomainExtentRAMSES(opt);
#ifdef USEHDF
    else if (opt.inputtype==IOHDF) MPIDomainExtentHDF(opt);
#endif
}

void MPIDomainDecomposition(Options &opt)
{
    MPIInitialDomainDecomposition();
    if(opt.inputtype==IOTIPSY) MPIDomainDecompositionTipsy(opt);
    else if (opt.inputtype==IOGADGET) MPIDomainDecompositionGadget(opt);
    else if (opt.inputtype==IORAMSES) MPIDomainDecompositionRAMSES(opt);
#ifdef USEHDF
    else if (opt.inputtype==IOHDF) MPIDomainDecompositionHDF(opt);
#endif
}

///adjust the domain boundaries to code units
void MPIAdjustDomain(Options opt){
    Double_t aadjust, lscale;
    if (opt.comove) aadjust=1.0;
    else aadjust=opt.a;
    lscale=opt.L/opt.h*aadjust;
    for (int j=0;j<NProcs;j++) for (int k=0;k<3;k++) {mpi_domain[j].bnd[k][0]*=lscale;mpi_domain[j].bnd[k][1]*=lscale;}
}

///given a position and a mpi thread domain information, determine which processor a particle is assigned to
int MPIGetParticlesProcessor(Double_t x,Double_t y, Double_t z){
    if (NProcs==1) return 0;
    for (int j=0;j<NProcs;j++){
        if( (mpi_domain[j].bnd[0][0]<=x) && (mpi_domain[j].bnd[0][1]>=x)&&
            (mpi_domain[j].bnd[1][0]<=y) && (mpi_domain[j].bnd[1][1]>=y)&&
            (mpi_domain[j].bnd[2][0]<=z) && (mpi_domain[j].bnd[2][1]>=z) )
            return j;
    }
    cerr<<ThisTask<<" has particle outside the mpi domains of every process ("<<x<<","<<y<<","<<z<<")"<<endl;
    MPI_Abort(MPI_COMM_WORLD,9);
}

//adds a particle read from an input file to the appropriate buffers
void MPIAddParticletoAppropriateBuffer(const int &ibuf, Int_t ibufindex, int *&ireadtask, const Int_t &BufSize, Int_t *&Nbuf, Particle *&Pbuf, Int_t &numpart, Particle *Part, Int_t *&Nreadbuf, vector<Particle>*&Preadbuf){
    if (ibuf==ThisTask) {
        Nbuf[ibuf]--;
        Part[numpart++]=Pbuf[ibufindex];
    }
    else {
        if(Nbuf[ibuf]==BufSize&&ireadtask[ibuf]<0) {
            MPI_Send(&Nbuf[ibuf], 1, MPI_Int_t, ibuf, ibuf+NProcs, MPI_COMM_WORLD);
            MPI_Send(&Pbuf[ibuf*BufSize],sizeof(Particle)*Nbuf[ibuf],MPI_BYTE,ibuf,ibuf,MPI_COMM_WORLD);
            Nbuf[ibuf]=0;
        }
        else if (ireadtask[ibuf]>=0) {
            if (ibuf!=ThisTask) {
                if (Nreadbuf[ireadtask[ibuf]]==Preadbuf[ireadtask[ibuf]].size()) Preadbuf[ireadtask[ibuf]].resize(Preadbuf[ireadtask[ibuf]].size()+BufSize);
                Preadbuf[ireadtask[ibuf]][Nreadbuf[ireadtask[ibuf]]]=Pbuf[ibufindex];
                Nreadbuf[ireadtask[ibuf]]++;
                Nbuf[ibuf]=0;
            }
        }
    }
}

//@}

/// \name routines which check to see if some search region overlaps with local mpi domain
//@{
///search if some region is in the local mpi domain
int MPIInDomain(Double_t xsearch[3][2], Double_t bnd[3][2]){
    Double_t xsearchp[3][2];
    if (NProcs==1) return 1;
    if (!((bnd[0][1] < xsearch[0][0]) || (bnd[0][0] > xsearch[0][1]) ||
        (bnd[1][1] < xsearch[1][0]) || (bnd[1][0] > xsearch[1][1]) ||
        (bnd[2][1] < xsearch[2][0]) || (bnd[2][0] > xsearch[2][1])))
            return 1;
    else {
        if (mpi_period==0) return 0;
        else {
            for (int j=0;j<3;j++) {xsearchp[j][0]=xsearch[j][0];xsearchp[j][1]=xsearch[j][1];}
            for (int j=0;j<3;j++) {
                if (!((bnd[j][1] < xsearch[j][0]+mpi_period) || (bnd[j][0] > xsearch[j][1]+mpi_period))) {xsearchp[j][0]+=mpi_period;xsearchp[j][1]+=mpi_period;}
                else if (!((bnd[j][1] < xsearch[j][0]-mpi_period) || (bnd[j][0] > xsearch[j][1]-mpi_period))) {xsearchp[j][0]-=mpi_period;xsearchp[j][1]-=mpi_period;}
            }
            if (!((bnd[0][1] < xsearchp[0][0]) || (bnd[0][0] > xsearchp[0][1]) ||
            (bnd[1][1] < xsearchp[1][0]) || (bnd[1][0] > xsearchp[1][1]) ||
            (bnd[2][1] < xsearchp[2][0]) || (bnd[2][0] > xsearchp[2][1])))
                return 1;
            else return 0;
        }
    }
}

///\todo clean up memory allocation in these functions, no need to keep allocating xsearch,xsearchp,numoverlap,etc
/// Determine if a particle needs to be exported to another mpi domain based on a physical search radius
int MPISearchForOverlap(Particle &Part, Double_t &rdist){
    Double_t xsearch[3][2];
    for (auto k=0;k<3;k++) {xsearch[k][0]=Part.GetPosition(k)-rdist;xsearch[k][1]=Part.GetPosition(k)+rdist;}
    return MPISearchForOverlap(xsearch);
}

int MPISearchForOverlap(Double_t xsearch[3][2]){
    Double_t xsearchp[7][3][2];//used to store periodic reflections
    int numoverlap=0,numreflecs=0,ireflec[3],numreflecchoice=0;
    int indomain;
    int j,k;

    for (j=0;j<NProcs;j++) {
        if (j!=ThisTask) {
            //determine if search region is not outside of this processors domain
            if(!((mpi_domain[j].bnd[0][1] < xsearch[0][0]) || (mpi_domain[j].bnd[0][0] > xsearch[0][1]) ||
                (mpi_domain[j].bnd[1][1] < xsearch[1][0]) || (mpi_domain[j].bnd[1][0] > xsearch[1][1]) ||
                (mpi_domain[j].bnd[2][1] < xsearch[2][0]) || (mpi_domain[j].bnd[2][0] > xsearch[2][1])))
                numoverlap++;
        }
    }
    if (mpi_period!=0) {
        for (k=0;k<3;k++) if (xsearch[k][0]<0||xsearch[k][1]>mpi_period) ireflec[numreflecs++]=k;
        if (numreflecs==1)numreflecchoice=1;
        else if (numreflecs==2) numreflecchoice=3;
        else if (numreflecs==3) numreflecchoice=7;
        for (j=0;j<numreflecchoice;j++) for (k=0;k<3;k++) {xsearchp[j][k][0]=xsearch[k][0];xsearchp[j][k][1]=xsearch[k][1];}
        if (numreflecs==1) {
            if (xsearch[ireflec[0]][0]<0) {
                    xsearchp[0][ireflec[0]][0]=xsearch[ireflec[0]][0]+mpi_period;
                    xsearchp[0][ireflec[0]][1]=xsearch[ireflec[0]][1]+mpi_period;
            }
            else if (xsearch[ireflec[0]][1]>mpi_period) {
                    xsearchp[0][ireflec[0]][0]=xsearch[ireflec[0]][0]-mpi_period;
                    xsearchp[0][ireflec[0]][1]=xsearch[ireflec[0]][1]-mpi_period;
            }
        }
        else if (numreflecs==2) {
            k=0;j=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;j++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
        }
        else if (numreflecs==3) {
            j=0;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k=1;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k=2;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=1;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k=2;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
        }

    for (j=0;j<NProcs;j++) for (k=0;k<numreflecchoice;k++){
        if (j!=ThisTask) {
            if(!((mpi_domain[j].bnd[0][1] < xsearchp[k][0][0]) || (mpi_domain[j].bnd[0][0] > xsearchp[k][0][1]) ||
            (mpi_domain[j].bnd[1][1] < xsearchp[k][1][0]) || (mpi_domain[j].bnd[1][0] > xsearchp[k][1][1]) ||
            (mpi_domain[j].bnd[2][1] < xsearchp[k][2][0]) || (mpi_domain[j].bnd[2][0] > xsearchp[k][2][1])))
            numoverlap++;
        }
    }
    }
    return numoverlap;
}

///\todo clean up memory allocation in these functions, no need to keep allocating xsearch,xsearchp,numoverlap,etc
/// Determine if a particle needs to be exported to another mpi domain based on a physical search radius
#ifdef SWIFTINTERFACE
int MPISearchForOverlapUsingMesh(Options &opt, Particle &Part, Double_t &rdist){
    Double_t xsearch[3][2];
    Double_t xsearchp[7][3][2];//used to store periodic reflections
    int numoverlap=0,numreflecs=0,ireflec[3],numreflecchoice=0;
    int indomain;
    int j,k;

    for (k=0;k<3;k++) {xsearch[k][0]=Part.GetPosition(k)-rdist;xsearch[k][1]=Part.GetPosition(k)+rdist;}

    /// Store whether an MPI domain has already been sent to
    int *sent_mpi_domain = new int[NProcs];
    for(int i=0; i<NProcs; i++) sent_mpi_domain[i] = 0;

    vector<int> celllist=MPIGetCellListInSearchUsingMesh(opt,xsearch);
    for (auto j:celllist) {
        const int cellnodeID = opt.cellnodeids[j];
        /// Only check if particles have overlap with neighbouring cells that are on another MPI domain and have not already been sent to
        if (sent_mpi_domain[cellnodeID] == 1) continue;
        numoverlap++;
        sent_mpi_domain[cellnodeID]++;
    }
    /*
    if (mpi_period!=0) {
        for (k=0;k<3;k++) if (xsearch[k][0]<0||xsearch[k][1]>mpi_period) ireflec[numreflecs++]=k;
        if (numreflecs==1)numreflecchoice=1;
        else if (numreflecs==2) numreflecchoice=3;
        else if (numreflecs==3) numreflecchoice=7;
        for (j=0;j<numreflecchoice;j++) for (k=0;k<3;k++) {xsearchp[j][k][0]=xsearch[k][0];xsearchp[j][k][1]=xsearch[k][1];}
        if (numreflecs==1) {
            if (xsearch[ireflec[0]][0]<0) {
                    xsearchp[0][ireflec[0]][0]=xsearch[ireflec[0]][0]+mpi_period;
                    xsearchp[0][ireflec[0]][1]=xsearch[ireflec[0]][1]+mpi_period;
            }
            else if (xsearch[ireflec[0]][1]>mpi_period) {
                    xsearchp[0][ireflec[0]][0]=xsearch[ireflec[0]][0]-mpi_period;
                    xsearchp[0][ireflec[0]][1]=xsearch[ireflec[0]][1]-mpi_period;
            }
        }
        else if (numreflecs==2) {
            k=0;j=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;j++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
        }
        else if (numreflecs==3) {
            j=0;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k=1;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k=2;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=1;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k=2;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
        }
        for (int k=0;k<numreflecchoice;k++) {
            vector<int> celllist=MPIGetCellListInSearchUsingMesh(opt,xsearchp[k]);
            for (auto j:celllist) {
                const int cellnodeID = opt.cellnodeids[j];
                /// Only check if particles have overlap with neighbouring cells that are on another MPI domain and have not already been sent to
                if (sent_mpi_domain[cellnodeID] == 1) continue;
                numoverlap++;
                sent_mpi_domain[cellnodeID]++;
            }
        }
    }
    */
    return numoverlap;
}
#endif
//@}

/// \name Routines involved in reading input data
//@{
///Distribute the mpi processes that read the input files so as to spread the read threads evenly throughout the MPI_COMM_WORLD
void MPIDistributeReadTasks(Options&opt, int *&ireadtask, int*&readtaskID){
    //initialize
    if (opt.nsnapread>NProcs) opt.nsnapread=NProcs;
    if (opt.num_files<opt.nsnapread) opt.nsnapread=opt.num_files;
    for (int i=0;i<NProcs;i++) ireadtask[i]=-1;
    int spacing=max(1,(int)floor(NProcs/opt.nsnapread));
    for (int i=0;i<opt.nsnapread;i++) {ireadtask[i*spacing]=i;readtaskID[i]=i*spacing;}
}

int MPISetFilesRead(Options&opt, int *&ireadfile, int *&ireadtask){
    //to determine which files the thread should read
    ireadfile=new int[opt.num_files];
    for (int i=0;i<opt.num_files;i++) ireadfile[i]=0;
    int nread=opt.num_files/opt.nsnapread;
    int niread=ireadtask[ThisTask]*nread,nfread=(ireadtask[ThisTask]+1)*nread;
    if (ireadtask[ThisTask]==opt.nsnapread-1) nfread=opt.num_files;
    for (int i=niread;i<nfread;i++) ireadfile[i]=1;
    return niread;
}
//@}

/// \name Routines involved in exporting particles
//@{
///for all threads not reading snapshots, simply receive particles as necessary from all threads involved with reading the data
void MPIReceiveParticlesFromReadThreads(Options &opt, Particle *&Pbuf, Particle *Part, int *&readtaskID, int *&irecv, int *&mpi_irecvflag, Int_t *&Nlocalthreadbuf, MPI_Request *&mpi_request, Particle *&Pbaryons)
{
    int irecvflag;
    Int_t i,j,k,Nlocaltotalbuf;
    MPI_Status status;
    //for all threads not reading snapshots, simply receive particles as necessary from all threads involved with reading the data
    //first determine which threads are going to send information to this thread.
    for (i=0;i<opt.nsnapread;i++) if (irecv[i]) {
        mpi_irecvflag[i]=0;
        MPI_Irecv(&Nlocalthreadbuf[i], 1, MPI_Int_t, readtaskID[i], ThisTask+NProcs, MPI_COMM_WORLD, &mpi_request[i]);
    }
    Nlocaltotalbuf=0;
    //non-blocking receives for the number of particles one expects to receive
    do {
        irecvflag=0;
        for (i=0;i<opt.nsnapread;i++) if (irecv[i]) {
            if (mpi_irecvflag[i]==0) {
                //test if a request has been sent for a Recv call by one of the read threads
                MPI_Test(&mpi_request[i], &mpi_irecvflag[i], &status);
                if (mpi_irecvflag[i]) {
                    if (Nlocalthreadbuf[i]>0) {
                        MPI_Recv(&Part[Nlocal],sizeof(Particle)*Nlocalthreadbuf[i],MPI_BYTE,readtaskID[i],ThisTask, MPI_COMM_WORLD,&status);
                        Nlocal+=Nlocalthreadbuf[i];
                        Nlocaltotalbuf+=Nlocalthreadbuf[i];
                        mpi_irecvflag[i]=0;
                        MPI_Irecv(&Nlocalthreadbuf[i], 1, MPI_Int_t, readtaskID[i], ThisTask+NProcs, MPI_COMM_WORLD, &mpi_request[i]);
                    }
                    else {
                        irecv[i]=0;
                    }
                }
            }
        }
        for (i=0;i<opt.nsnapread;i++) irecvflag+=irecv[i];
    } while(irecvflag>0);
    //now that data is local, must adjust data iff a separate baryon search is required.
    if (opt.partsearchtype==PSTDARK && opt.iBaryonSearch) {
        for (i=0;i<Nlocal;i++) {
            k=Part[i].GetType();
            if (!(k==GASTYPE||k==STARTYPE||k==BHTYPE)) Part[i].SetID(0);
            else {
                Nlocalbaryon[0]++;
                if  (k==GASTYPE) {Part[i].SetID(1);Nlocalbaryon[1]++;}
                else if  (k==STARTYPE) {Part[i].SetID(2);Nlocalbaryon[2]++;}
                else if  (k==BHTYPE) {Part[i].SetID(3);Nlocalbaryon[3]++;}
            }
        }
        //sorted so that dark matter particles first, baryons after
        qsort(Part,Nlocal, sizeof(Particle), IDCompare);
        //sort(Part.begin(),Part.end(), IDCompareVec);
        Nlocal-=Nlocalbaryon[0];
        //index type separated
        for (i=0;i<Nlocal;i++) Part[i].SetID(i);
        for (i=0;i<Nlocalbaryon[0];i++) Part[i+Nlocal].SetID(i+Nlocal);
    }
}

/*! Final send between read threads of input particle data
*/
void MPISendParticlesBetweenReadThreads(Options &opt, Particle *&Pbuf, Particle *Part, Int_t *&nreadoffset, int *&ireadtask, int *&readtaskID, Particle *&Pbaryons, Int_t *&mpi_nsend_baryon)
{
    if (ireadtask[ThisTask]>=0) {
        //split the communication into small buffers
        int icycle=0,ibuf;
        //maximum send size
        int maxchunksize=2147483648/opt.nsnapread/sizeof(Particle);
        int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
        int sendTask,recvTask;
        int sendoffset,recvoffset;
        int isendrecv;
        int cursendchunksize,currecvchunksize;
        MPI_Status status;
        for (ibuf=0;ibuf< opt.nsnapread; ibuf++){
            //if there are an even number of read tasks, then communicate such that 0 communcates with N-1, 1<->N-2, etc
            //and moves on to next communication 0<->N-2, 1<->N-3, etc with the communication in chunks
            //first base on read thread position
            sendTask=ireadtask[ThisTask];
            ///map so that 0 <->N-1, 1 <->N-2, etc to start moving to
            recvTask=abs(opt.nsnapread-1-ibuf-sendTask);
            //if have cycled passed zero, then need to adjust recvTask
            if (icycle==1) recvTask=opt.nsnapread-recvTask;

            //now adjust to the actually task ID in the MPI_COMM_WORLD
            sendTask=ThisTask;
            recvTask=readtaskID[recvTask];
            //if ibuf>0 and now at recvTask=0, then next time, cycle
            if (ibuf>0 && recvTask==0) icycle=1;
            //if sendtask!=recvtask, and information needs to be sent, send information
            if (sendTask!=recvTask && (mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)) {
                nsend=mpi_nsend[ThisTask * NProcs + recvTask];
                nrecv=mpi_nsend[recvTask * NProcs + ThisTask];
                //calculate how many send/recvs are needed
                nsendchunks=ceil((double)nsend/(double)maxchunksize);
                nrecvchunks=ceil((double)nrecv/(double)maxchunksize);
                numsendrecv=max(nsendchunks,nrecvchunks);
                //initialize the offset in the particle array
                sendoffset=0;
                recvoffset=0;
                isendrecv=1;
                do
                {
                    //determine amount to be sent
                    cursendchunksize=min(maxchunksize,nsend-sendoffset);
                    currecvchunksize=min(maxchunksize,nrecv-recvoffset);
                    //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                    //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                    MPI_Sendrecv(&Pbuf[nreadoffset[ireadtask[recvTask]]+sendoffset],sizeof(Particle)*cursendchunksize, MPI_BYTE, recvTask, TAG_IO_A+isendrecv,
                        &Part[Nlocal],sizeof(Particle)*currecvchunksize, MPI_BYTE, recvTask, TAG_IO_A+isendrecv,
                                MPI_COMM_WORLD, &status);
                    Nlocal+=currecvchunksize;
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    isendrecv++;
                } while (isendrecv<=numsendrecv);
            }
            //if separate baryon search, send baryons too
            if (opt.iBaryonSearch && opt.partsearchtype!=PSTALL) {
                nsend=mpi_nsend_baryon[ThisTask * NProcs + recvTask];
                nrecv=mpi_nsend_baryon[recvTask * NProcs + ThisTask];
                //calculate how many send/recvs are needed
                nsendchunks=ceil((double)nsend/(double)maxchunksize);
                nrecvchunks=ceil((double)nrecv/(double)maxchunksize);
                numsendrecv=max(nsendchunks,nrecvchunks);
                //initialize the offset in the particle array
                sendoffset=0;
                recvoffset=0;
                isendrecv=1;
                do
                {
                    //determine amount to be sent
                    cursendchunksize=min(maxchunksize,nsend-sendoffset);
                    currecvchunksize=min(maxchunksize,nrecv-recvoffset);
                    //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                    //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                    MPI_Sendrecv(&Pbuf[nreadoffset[ireadtask[recvTask]]+mpi_nsend[ThisTask * NProcs + recvTask]+sendoffset],sizeof(Particle)*cursendchunksize, MPI_BYTE, recvTask, TAG_IO_B+isendrecv,
                        &Pbaryons[Nlocalbaryon[0]],sizeof(Particle)*currecvchunksize, MPI_BYTE, recvTask, TAG_IO_B+isendrecv,
                                MPI_COMM_WORLD, &status);
                    Nlocalbaryon[0]+=currecvchunksize;
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    isendrecv++;
                } while (isendrecv<=numsendrecv);
            }
        }
    }
}

void MPISendParticlesBetweenReadThreads(Options &opt, vector<Particle> *&Preadbuf, Particle *Part, int *&ireadtask, int *&readtaskID, Particle *&Pbaryons, MPI_Comm &mpi_comm_read, Int_t *&mpi_nsend_readthread, Int_t *&mpi_nsend_readthread_baryon)
{
    if (ireadtask[ThisTask]>=0) {
        //split the communication into small buffers
        int icycle=0,ibuf;
        //maximum send size
        int maxchunksize=2147483648/opt.nsnapread/sizeof(Particle);
        int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
        int sendTask,recvTask;
        int sendoffset,recvoffset;
        int isendrecv;
        int cursendchunksize,currecvchunksize;
        MPI_Status status;
        for (ibuf=0;ibuf< opt.nsnapread; ibuf++){
            //if there are an even number of read tasks, then communicate such that 0 communcates with N-1, 1<->N-2, etc
            //and moves on to next communication 0<->N-2, 1<->N-3, etc with the communication in chunks
            //first base on read thread position
            sendTask=ireadtask[ThisTask];
            ///map so that 0 <->N-1, 1 <->N-2, etc to start moving to
            recvTask=abs(opt.nsnapread-1-ibuf-sendTask);
            //if have cycled passed zero, then need to adjust recvTask
            if (icycle==1) recvTask=opt.nsnapread-recvTask;

            //if ibuf>0 and now at recvTask=0, then next time, cycle
            if (ibuf>0 && recvTask==0) icycle=1;
            //if sendtask!=recvtask, and information needs to be sent, send information
            if (sendTask!=recvTask && (mpi_nsend_readthread[sendTask * opt.nsnapread + recvTask] > 0 || mpi_nsend_readthread[recvTask * opt.nsnapread + sendTask] > 0)) {
                nsend=mpi_nsend_readthread[sendTask * opt.nsnapread + recvTask];
                nrecv=mpi_nsend_readthread[recvTask * opt.nsnapread + sendTask];
                //calculate how many send/recvs are needed
                nsendchunks=ceil((double)nsend/(double)maxchunksize);
                nrecvchunks=ceil((double)nrecv/(double)maxchunksize);
                numsendrecv=max(nsendchunks,nrecvchunks);
                //initialize the offset in the particle array
                sendoffset=0;
                recvoffset=0;
                isendrecv=1;
                do
                {
                    //determine amount to be sent
                    cursendchunksize=min(maxchunksize,nsend-sendoffset);
                    currecvchunksize=min(maxchunksize,nrecv-recvoffset);
                    //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                    //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                    MPI_Sendrecv(&Preadbuf[recvTask][sendoffset],sizeof(Particle)*cursendchunksize, MPI_BYTE, recvTask, TAG_IO_A+isendrecv,
                        &(Part[Nlocal]),sizeof(Particle)*currecvchunksize, MPI_BYTE, recvTask, TAG_IO_A+isendrecv,
                                mpi_comm_read, &status);
                    Nlocal+=currecvchunksize;
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    isendrecv++;
                } while (isendrecv<=numsendrecv);
            }
            //if separate baryon search, send baryons too
            if (opt.iBaryonSearch && opt.partsearchtype!=PSTALL) {
                nsend=mpi_nsend_readthread_baryon[sendTask * opt.nsnapread + recvTask];
                nrecv=mpi_nsend_readthread_baryon[recvTask * opt.nsnapread + sendTask];
                //calculate how many send/recvs are needed
                nsendchunks=ceil((double)nsend/(double)maxchunksize);
                nrecvchunks=ceil((double)nrecv/(double)maxchunksize);
                numsendrecv=max(nsendchunks,nrecvchunks);
                //initialize the offset in the particle array
                sendoffset=0;
                recvoffset=0;
                isendrecv=1;
                do
                {
                    //determine amount to be sent
                    cursendchunksize=min(maxchunksize,nsend-sendoffset);
                    currecvchunksize=min(maxchunksize,nrecv-recvoffset);
                    //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                    //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                    MPI_Sendrecv(&Preadbuf[recvTask][mpi_nsend_readthread[sendTask * opt.nsnapread + recvTask]+sendoffset],sizeof(Particle)*cursendchunksize, MPI_BYTE, recvTask, TAG_IO_B+isendrecv,
                        &Pbaryons[Nlocalbaryon[0]],sizeof(Particle)*currecvchunksize, MPI_BYTE, recvTask, TAG_IO_B+isendrecv,
                                mpi_comm_read, &status);
                    Nlocalbaryon[0]+=currecvchunksize;
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    isendrecv++;
                } while (isendrecv<=numsendrecv);
            }
        }
    }
}

void MPIGetExportNum(const Int_t nbodies, Particle *Part, Double_t rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++) {
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist;xsearch[k][1]=Part[i].GetPosition(k)+rdist;}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    NExport=nexport;//*(1.0+MPIExportFac);
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;
    for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
}

#ifdef SWIFTINTERFACE
void MPIGetExportNumUsingMesh(Options &opt, const Int_t nbodies, Particle *Part, Double_t rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;
    //siminfo *s = &opt.swiftsiminfo;
    //Options *s = &opt;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;

    cout<<"Finding number of particles to export to other MPI domains..."<<endl;

    /// Get some constants
    const double dim_x = opt.spacedimension[0];
    const double dim_y = opt.spacedimension[1];
    const double dim_z = opt.spacedimension[2];
    const int cdim[3] = {opt.numcellsperdim, opt.numcellsperdim, opt.numcellsperdim};
    const double ih_x = opt.icellwidth[0];
    const double ih_y = opt.icellwidth[1];
    const double ih_z = opt.icellwidth[2];
    int *sent_mpi_domain = new int[NProcs];

    for (i=0;i<nbodies;i++) {
        for(int k=0; k<NProcs; k++) sent_mpi_domain[k] = 0;
        for(int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist;xsearch[k][1]=Part[i].GetPosition(k)+rdist;}
        vector<int> celllist=MPIGetCellListInSearchUsingMesh(opt,xsearch);
        for (auto j:celllist) {
            const int cellnodeID = opt.cellnodeids[j];
            /// Only check if particles have overlap with neighbouring cells that are on another MPI domain and have not already been sent to
            if (sent_mpi_domain[cellnodeID] == 1) continue;
            vector<int> celllist=MPIGetCellListInSearchUsingMesh(opt,xsearch);
            for (auto j:celllist) {
                const int cellnodeID = opt.cellnodeids[j];
                /// Only check if particles have overlap with neighbouring cells that are on another MPI domain and have not already been sent to
                if (sent_mpi_domain[cellnodeID] == 1) continue;
                nexport++;
                nsend_local[cellnodeID]++;
                sent_mpi_domain[cellnodeID]++;
            }
        }
    }
    NExport=nexport;//*(1.0+MPIExportFac);
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;
    for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
}
#endif

/*! Determine which particles have a spatial linking length such that linking overlaps the domain of another processor store the necessary information to send that data
    and then send that information
*/
void MPIBuildParticleExportList(const Int_t nbodies, Particle *Part, Int_t *&pfof, Int_tree_t *&Len, Double_t rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++) {
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist;xsearch[k][1]=Part[i].GetPosition(k)+rdist;}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    //FoFDataIn[nexport].Part=Part[i];
                    FoFDataIn[nexport].Index = i;
                    FoFDataIn[nexport].Task = j;
                    FoFDataIn[nexport].iGroup = pfof[Part[i].GetID()];//set group id
                    FoFDataIn[nexport].iGroupTask = ThisTask;//and the task of the group
                    FoFDataIn[nexport].iLen = Len[i];
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    if (nexport>0) {
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number
    qsort(FoFDataIn, nexport, sizeof(struct fofdata_in), fof_export_cmp);
    for (i=0;i<nexport;i++) PartDataIn[i] = Part[FoFDataIn[i].Index];
    }
    //then store the offset in the export particle data for the jth Task in order to send data.
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    //now send the data.
    for (j=0;j<NProcs;j++)nimport+=mpi_nsend[ThisTask+j*NProcs];

    //check if buffer that needs to be send is too large and must be sent in chunks
    int bufferFlag = 1;
    long int maxNumPart = LOCAL_MAX_MSGSIZE / (long int) sizeof(Particle);
    for (j = 0; j < NProcs; j++)
    {
        if (j != ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;
            if (mpi_nsend[ThisTask+recvTask*NProcs] >= maxNumPart || nsend_local[recvTask] >= maxNumPart ) bufferFlag++;
        }
    }
    //if buffer is too large, split sends
    if (bufferFlag)
    {
        MPI_Request rqst;
        int numBuffersToSend [NProcs];
        int numBuffersToRecv [NProcs];
        int numPartInBuffer = maxNumPart * 0.9;
        int maxnbufferslocal=0,maxnbuffers;
        for (j = 0; j < NProcs; j++)
        {
            numBuffersToSend[j] = 0;
            numBuffersToRecv[j] = 0;
            if (nsend_local[j] > 0)
            numBuffersToSend[j] = (nsend_local[j]/numPartInBuffer) + 1;
        }
        for (int i = 1; i < NProcs; i++)
        {
            int src = (ThisTask + NProcs - i) % NProcs;
            int dst = (ThisTask + i) % NProcs;
            MPI_Isend (&numBuffersToSend[dst], 1, MPI_INT, dst, 0, MPI_COMM_WORLD, &rqst);
            MPI_Recv  (&numBuffersToRecv[src], 1, MPI_INT, src, 0, MPI_COMM_WORLD, &status);
        }
        MPI_Barrier (MPI_COMM_WORLD);
        //find max to be transfer, allows appropriate tagging of messages
        for (int i=0;i<NProcs;i++) if (numBuffersToRecv[i]>maxnbufferslocal) maxnbufferslocal=numBuffersToRecv[i];
        for (int i=0;i<NProcs;i++) if (numBuffersToSend[i]>maxnbufferslocal) maxnbufferslocal=numBuffersToSend[i];
        MPI_Allreduce (&maxnbufferslocal, &maxnbuffers, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);

        for (int i = 1; i < NProcs; i++)
        {
            int src = (ThisTask + NProcs - i) % NProcs;
            int dst = (ThisTask + i) % NProcs;
            Int_t size = numPartInBuffer;
            nbuffer[src] = 0;
            int buffOffset = 0;

            for (int jj = 0; jj < src; jj++)  nbuffer[src] += mpi_nsend[ThisTask + jj*NProcs];

            // Send Buffers
            for (int jj = 0; jj < numBuffersToSend[dst]-1; jj++)
            {
                MPI_Isend (&size, 1, MPI_Int_t, dst, (int)(jj+1), MPI_COMM_WORLD, &rqst);
                MPI_Isend (&FoFDataIn[noffset[dst] + buffOffset], sizeof(struct fofdata_in)*size,
                            MPI_BYTE, dst, (int)(TAG_FOF_A*maxnbuffers+jj+1), MPI_COMM_WORLD, &rqst);
                MPI_Isend (&PartDataIn[noffset[dst] + buffOffset], sizeof(Particle)*size,
                            MPI_BYTE, dst, (int)(TAG_FOF_B*maxnbuffers*3+jj+1), MPI_COMM_WORLD, &rqst);
                buffOffset += size;
            }
            size = nsend_local[dst] % numPartInBuffer;
            if (size > 0 && numBuffersToSend[dst] > 0)
            {
                MPI_Isend (&size, 1, MPI_Int_t, dst, (int)(numBuffersToSend[dst]), MPI_COMM_WORLD, &rqst);
                MPI_Isend (&FoFDataIn[noffset[dst] + buffOffset], sizeof(struct fofdata_in)*size,
                            MPI_BYTE, dst, (int)(TAG_FOF_A*maxnbuffers+numBuffersToSend[dst]), MPI_COMM_WORLD, &rqst);
                MPI_Isend (&PartDataIn[noffset[dst] + buffOffset], sizeof(Particle)*size,
                            MPI_BYTE, dst, (int)(TAG_FOF_B*maxnbuffers*3+numBuffersToSend[dst]), MPI_COMM_WORLD, &rqst);
            }
            // Receive Buffers
            buffOffset = 0;
            for (int jj = 0; jj < numBuffersToRecv[src]; jj++)
            {
                Int_t numInBuffer = 0;
                MPI_Recv (&numInBuffer, 1, MPI_Int_t, src, (int)(jj+1), MPI_COMM_WORLD, &status);
                MPI_Recv (&FoFDataGet[nbuffer[src] + buffOffset], sizeof(struct fofdata_in)*numInBuffer,
                            MPI_BYTE, src, (int)(TAG_FOF_A*maxnbuffers+jj+1), MPI_COMM_WORLD, &status);
                MPI_Recv (&PartDataGet[nbuffer[src] + buffOffset], sizeof(Particle)*numInBuffer,
                            MPI_BYTE, src, (int)(TAG_FOF_B*maxnbuffers*3+jj+1), MPI_COMM_WORLD, &status);
                buffOffset += numInBuffer;
            }
        }
    }
    else
    {
        if (nexport>0||nimport>0) {
            for(j=0;j<NProcs;j++)//for(j=1;j<NProcs;j++)
            {
                if (j!=ThisTask)
                {
                    sendTask = ThisTask;
                    recvTask = j;//ThisTask^j;//bitwise XOR ensures that recvTask cycles around sendTask
                    nbuffer[recvTask]=0;
                    for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer
                    if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
                    {
                        //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                        //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                        //first send FOF data and then particle data
                        MPI_Sendrecv(&FoFDataIn[noffset[recvTask]],
                            nsend_local[recvTask] * sizeof(struct fofdata_in), MPI_BYTE,
                            recvTask, TAG_FOF_A,
                            &FoFDataGet[nbuffer[recvTask]],
                            mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct fofdata_in),
                            MPI_BYTE, recvTask, TAG_FOF_A, MPI_COMM_WORLD, &status);
                        MPI_Sendrecv(&PartDataIn[noffset[recvTask]],
                            nsend_local[recvTask] * sizeof(Particle), MPI_BYTE,
                            recvTask, TAG_FOF_B,
                            &PartDataGet[nbuffer[recvTask]],
                            mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(Particle),
                            MPI_BYTE, recvTask, TAG_FOF_B, MPI_COMM_WORLD, &status);
                    }
                }
            }
        }
    }
}

/*! Similar to \ref MPIBuildParticleExportList but uses mesh of swift to determine when mpi's to search
*/
#ifdef SWIFTINTERFACE
void MPIBuildParticleExportListUsingMesh(Options &opt, const Int_t nbodies, Particle *Part, Int_t *&pfof, Int_tree_t *&Len, Double_t rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;
    int *sent_mpi_domain = new int[NProcs];

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;

    cout<<ThisTask<<" now building exported particle list for FOF search "<<endl;
    for (i=0;i<nbodies;i++) {
        for(int k=0; k<NProcs; k++) sent_mpi_domain[k] = 0;
        for(int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist;xsearch[k][1]=Part[i].GetPosition(k)+rdist;}
        vector<int> celllist=MPIGetCellListInSearchUsingMesh(opt,xsearch);
        for (auto j:celllist) {
            const int cellnodeID = opt.cellnodeids[j];
            /// Only check if particles have overlap with neighbouring cells that are on another MPI domain and have not already been sent to
            if (sent_mpi_domain[cellnodeID] == 1) continue;
            vector<int> celllist=MPIGetCellListInSearchUsingMesh(opt,xsearch);
            for (auto j:celllist) {
                const int cellnodeID = opt.cellnodeids[j];
                /// Only check if particles have overlap with neighbouring cells that are on another MPI domain and have not already been sent to
                if (sent_mpi_domain[cellnodeID] == 1) continue;
                //FoFDataIn[nexport].Part=Part[i];
                FoFDataIn[nexport].Index = i;
                FoFDataIn[nexport].Task = cellnodeID;
                FoFDataIn[nexport].iGroup = pfof[Part[i].GetID()];//set group id
                FoFDataIn[nexport].iGroupTask = ThisTask;//and the task of the group
                FoFDataIn[nexport].iLen = Len[i];
                nexport++;
                nsend_local[cellnodeID]++;
                sent_mpi_domain[cellnodeID]++;
            }
        }
    }

    if (nexport>0) {
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number
    qsort(FoFDataIn, nexport, sizeof(struct fofdata_in), fof_export_cmp);
    for (i=0;i<nexport;i++) PartDataIn[i] = Part[FoFDataIn[i].Index];
    }
    //then store the offset in the export particle data for the jth Task in order to send data.
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    //now send the data.
    for (j=0;j<NProcs;j++)nimport+=mpi_nsend[ThisTask+j*NProcs];

    //check if buffer that needs to be send is too large and must be sent in chunks
    int bufferFlag = 1;
    long int maxNumPart = LOCAL_MAX_MSGSIZE / (long int) sizeof(Particle);
    for (j = 0; j < NProcs; j++)
    {
        if (j != ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;
            if (mpi_nsend[ThisTask+recvTask*NProcs] >= maxNumPart || nsend_local[recvTask] >= maxNumPart ) bufferFlag++;
        }
    }
    //if buffer is too large, split sends
    if (bufferFlag)
    {
        MPI_Request rqst;
        int numBuffersToSend [NProcs];
        int numBuffersToRecv [NProcs];
        int numPartInBuffer = maxNumPart * 0.9;
        int maxnbufferslocal=0,maxnbuffers;
        for (j = 0; j < NProcs; j++)
        {
            numBuffersToSend[j] = 0;
            numBuffersToRecv[j] = 0;
            if (nsend_local[j] > 0)
            numBuffersToSend[j] = (nsend_local[j]/numPartInBuffer) + 1;
        }
        for (int i = 1; i < NProcs; i++)
        {
            int src = (ThisTask + NProcs - i) % NProcs;
            int dst = (ThisTask + i) % NProcs;
            MPI_Isend (&numBuffersToSend[dst], 1, MPI_INT, dst, 0, MPI_COMM_WORLD, &rqst);
            MPI_Recv  (&numBuffersToRecv[src], 1, MPI_INT, src, 0, MPI_COMM_WORLD, &status);
        }
        MPI_Barrier (MPI_COMM_WORLD);
        //find max to be transfer, allows appropriate tagging of messages
        for (int i=0;i<NProcs;i++) if (numBuffersToRecv[i]>maxnbufferslocal) maxnbufferslocal=numBuffersToRecv[i];
        for (int i=0;i<NProcs;i++) if (numBuffersToSend[i]>maxnbufferslocal) maxnbufferslocal=numBuffersToSend[i];
        MPI_Allreduce (&maxnbufferslocal, &maxnbuffers, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);

        for (int i = 1; i < NProcs; i++)
        {
            int src = (ThisTask + NProcs - i) % NProcs;
            int dst = (ThisTask + i) % NProcs;
            Int_t size = numPartInBuffer;
            nbuffer[src] = 0;
            int buffOffset = 0;

            for (int jj = 0; jj < src; jj++)  nbuffer[src] += mpi_nsend[ThisTask + jj*NProcs];

            // Send Buffers
            for (int jj = 0; jj < numBuffersToSend[dst]-1; jj++)
            {
                MPI_Isend (&size, 1, MPI_Int_t, dst, (int)(jj+1), MPI_COMM_WORLD, &rqst);
                MPI_Isend (&FoFDataIn[noffset[dst] + buffOffset], sizeof(struct fofdata_in)*size,
                            MPI_BYTE, dst, (int)(TAG_FOF_A*maxnbuffers+jj+1), MPI_COMM_WORLD, &rqst);
                MPI_Isend (&PartDataIn[noffset[dst] + buffOffset], sizeof(Particle)*size,
                            MPI_BYTE, dst, (int)(TAG_FOF_B*maxnbuffers*3+jj+1), MPI_COMM_WORLD, &rqst);
                buffOffset += size;
            }
            size = nsend_local[dst] % numPartInBuffer;
            if (size > 0 && numBuffersToSend[dst] > 0)
            {
                MPI_Isend (&size, 1, MPI_Int_t, dst, (int)(numBuffersToSend[dst]), MPI_COMM_WORLD, &rqst);
                MPI_Isend (&FoFDataIn[noffset[dst] + buffOffset], sizeof(struct fofdata_in)*size,
                            MPI_BYTE, dst, (int)(TAG_FOF_A*maxnbuffers+numBuffersToSend[dst]), MPI_COMM_WORLD, &rqst);
                MPI_Isend (&PartDataIn[noffset[dst] + buffOffset], sizeof(Particle)*size,
                            MPI_BYTE, dst, (int)(TAG_FOF_B*maxnbuffers*3+numBuffersToSend[dst]), MPI_COMM_WORLD, &rqst);
            }
            // Receive Buffers
            buffOffset = 0;
            for (int jj = 0; jj < numBuffersToRecv[src]; jj++)
            {
                Int_t numInBuffer = 0;
                MPI_Recv (&numInBuffer, 1, MPI_Int_t, src, (int)(jj+1), MPI_COMM_WORLD, &status);
                MPI_Recv (&FoFDataGet[nbuffer[src] + buffOffset], sizeof(struct fofdata_in)*numInBuffer,
                            MPI_BYTE, src, (int)(TAG_FOF_A*maxnbuffers+jj+1), MPI_COMM_WORLD, &status);
                MPI_Recv (&PartDataGet[nbuffer[src] + buffOffset], sizeof(Particle)*numInBuffer,
                            MPI_BYTE, src, (int)(TAG_FOF_B*maxnbuffers*3+jj+1), MPI_COMM_WORLD, &status);
                buffOffset += numInBuffer;
            }
        }
    }
    else
    {
        if (nexport>0||nimport>0) {
            for(j=0;j<NProcs;j++)//for(j=1;j<NProcs;j++)
            {
                if (j!=ThisTask)
                {
                    sendTask = ThisTask;
                    recvTask = j;//ThisTask^j;//bitwise XOR ensures that recvTask cycles around sendTask
                    nbuffer[recvTask]=0;
                    for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer
                    if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
                    {
                        //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                        //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                        //first send FOF data and then particle data
                        MPI_Sendrecv(&FoFDataIn[noffset[recvTask]],
                            nsend_local[recvTask] * sizeof(struct fofdata_in), MPI_BYTE,
                            recvTask, TAG_FOF_A,
                            &FoFDataGet[nbuffer[recvTask]],
                            mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct fofdata_in),
                            MPI_BYTE, recvTask, TAG_FOF_A, MPI_COMM_WORLD, &status);
                        MPI_Sendrecv(&PartDataIn[noffset[recvTask]],
                            nsend_local[recvTask] * sizeof(Particle), MPI_BYTE,
                            recvTask, TAG_FOF_B,
                            &PartDataGet[nbuffer[recvTask]],
                            mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(Particle),
                            MPI_BYTE, recvTask, TAG_FOF_B, MPI_COMM_WORLD, &status);
                    }
                }
            }
        }
    }
}
#endif

/*! like \ref MPIGetExportNum but number based on NN search, useful for reducing memory costs at the expense of cpu cycles
*/
void MPIGetNNExportNum(const Int_t nbodies, Particle *Part, Double_t *rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;
    int indomain;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++)
    {
#ifdef STRUCDEN
    if (Part[i].GetType()>0)
    {
#endif
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist[i];xsearch[k][1]=Part[i].GetPosition(k)+rdist[i];}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
#ifdef STRUCDEN
    }
#endif
    }
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;
    for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    NExport=nexport;
}

/*! like \ref MPIGetExportNum but number based on NN search, useful for reducing memory costs at the expense of cpu cycles
*/
#ifdef SWIFTINTERFACE
void MPIGetNNExportNumUsingMesh(Options &opt, const Int_t nbodies, Particle *Part, Double_t *rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;
    int indomain;
    int *sent_mpi_domain = new int[NProcs];

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++)
    {
#ifdef STRUCDEN
    if (Part[i].GetType()>0)
    {
#endif
        for(int k=0; k<NProcs; k++) sent_mpi_domain[k] = 0;
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist[i];xsearch[k][1]=Part[i].GetPosition(k)+rdist[i];}
        vector<int> celllist=MPIGetCellListInSearchUsingMesh(opt,xsearch);
        for (auto j:celllist) {
            const int cellnodeID = opt.cellnodeids[j];
            /// Only check if particles have overlap with neighbouring cells that are on another MPI domain and have not already been sent to
            if (sent_mpi_domain[cellnodeID] == 1) continue;
            vector<int> celllist=MPIGetCellListInSearchUsingMesh(opt,xsearch);
            for (auto j:celllist) {
                const int cellnodeID = opt.cellnodeids[j];
                /// Only check if particles have overlap with neighbouring cells that are on another MPI domain and have not already been sent to
                if (sent_mpi_domain[cellnodeID] == 1) continue;
                nexport++;
                nsend_local[cellnodeID]++;
                sent_mpi_domain[cellnodeID]++;
            }
        }
#ifdef STRUCDEN
    }
#endif
    }
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;
    for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    NExport=nexport;
}
#endif

/*! like \ref MPIBuildParticleExportList but each particle has a different distance stored in rdist used to find nearest neighbours
*/
void MPIBuildParticleNNExportList(const Int_t nbodies, Particle *Part, Double_t *rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    MPI_Status status;
    int indomain;
    int sendTask,recvTask;
    int maxchunksize=2147483648/NProcs/sizeof(nndata_in);
    int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
    int sendoffset,recvoffset;
    int cursendchunksize,currecvchunksize;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++)
#ifdef STRUCDEN
    if (Part[i].GetType()>0)
    {
#endif
    {
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist[i];xsearch[k][1]=Part[i].GetPosition(k)+rdist[i];}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    //NNDataIn[nexport].Index=i;
                    NNDataIn[nexport].ToTask=j;
                    NNDataIn[nexport].FromTask=ThisTask;
                    NNDataIn[nexport].R2=rdist[i]*rdist[i];
                    //NNDataIn[nexport].V2=vdist2[i];
                    for (int k=0;k<3;k++) {
                        NNDataIn[nexport].Pos[k]=Part[i].GetPosition(k);
                        NNDataIn[nexport].Vel[k]=Part[i].GetVelocity(k);
                    }
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
#ifdef STRUCDEN
    }
#endif
    }
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number
    if (nexport>0) qsort(NNDataIn, nexport, sizeof(struct nndata_in), nn_export_cmp);

    //then store the offset in the export particle data for the jth Task in order to send data.
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    //now send the data.
    ///\todo In determination of particle export, eventually need to place a check for the communication buffer so that if exported number
    ///is larger than the size of the buffer, iterate over the number exported
    //if either sending or receiving then run this process
    for (j=0;j<NProcs;j++)nimport+=mpi_nsend[ThisTask+j*NProcs];
    if (nexport>0||nimport>0) {
    for(j=0;j<NProcs;j++)//for(j=1;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;//ThisTask^j;
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //send info in loops to minimize memory footprint
                cursendchunksize=currecvchunksize=maxchunksize;
                nsendchunks=ceil(mpi_nsend[recvTask+ThisTask*NProcs]/(Double_t)maxchunksize);
                nrecvchunks=ceil(mpi_nsend[ThisTask+recvTask*NProcs]/(Double_t)maxchunksize);
                if (cursendchunksize>mpi_nsend[recvTask+ThisTask*NProcs]) {
                    nsendchunks=1;
                    cursendchunksize=mpi_nsend[recvTask+ThisTask*NProcs];
                }
                if (currecvchunksize>mpi_nsend[ThisTask+recvTask*NProcs]) {
                    nrecvchunks=1;
                    currecvchunksize=mpi_nsend[ThisTask+recvTask*NProcs];
                }
                numsendrecv=max(nsendchunks,nrecvchunks);
                sendoffset=recvoffset=0;
                for (auto ichunk=0;ichunk<numsendrecv;ichunk++)
                {
                    //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                    //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                    MPI_Sendrecv(&NNDataIn[noffset[recvTask]+sendoffset],
                        cursendchunksize * sizeof(struct nndata_in), MPI_BYTE,
                        recvTask, TAG_NN_A+ichunk,
                        &NNDataGet[nbuffer[recvTask]+recvoffset],
                        currecvchunksize * sizeof(struct nndata_in),
                        MPI_BYTE, recvTask, TAG_NN_A+ichunk, MPI_COMM_WORLD, &status);
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    if (cursendchunksize>mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset)cursendchunksize=mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset;
                    if (currecvchunksize>mpi_nsend[ThisTask+recvTask * NProcs]-sendoffset)currecvchunksize=mpi_nsend[ThisTask+recvTask * NProcs]-recvoffset;
                }
            }
        }
    }
    }
    MPI_Barrier(MPI_COMM_WORLD);
}
/*! like \ref MPIBuildParticleExportList but each particle has a different distance stored in rdist used to find nearest neighbours
*/
#ifdef SWIFTINTERFACE
void MPIBuildParticleNNExportListUsingMesh(Options &opt, const Int_t nbodies, Particle *Part, Double_t *rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;
    int indomain;
    int *sent_mpi_domain = new int[NProcs];

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++)
#ifdef STRUCDEN
    if (Part[i].GetType()>0)
    {
#endif
    {
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist[i];xsearch[k][1]=Part[i].GetPosition(k)+rdist[i];}

        /// Store whether an MPI domain has already been sent to
        for(int k=0; k<NProcs; k++) sent_mpi_domain[k] = 0;
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist[i];xsearch[k][1]=Part[i].GetPosition(k)+rdist[i];}
        vector<int> celllist=MPIGetCellListInSearchUsingMesh(opt,xsearch);
        for (auto j:celllist) {
            const int cellnodeID = opt.cellnodeids[j];
            /// Only check if particles have overlap with neighbouring cells that are on another MPI domain and have not already been sent to
            if (sent_mpi_domain[cellnodeID] == 1) continue;
            vector<int> celllist=MPIGetCellListInSearchUsingMesh(opt,xsearch);
            for (auto j:celllist) {
                const int cellnodeID = opt.cellnodeids[j];
                /// Only check if particles have overlap with neighbouring cells that are on another MPI domain and have not already been sent to
                if (sent_mpi_domain[cellnodeID] == 1) continue;
                //NNDataIn[nexport].Index=i;
                NNDataIn[nexport].ToTask=cellnodeID;
                NNDataIn[nexport].FromTask=ThisTask;
                NNDataIn[nexport].R2=rdist[i]*rdist[i];
                //NNDataIn[nexport].V2=vdist2[i];
                for (int k=0;k<3;k++) {
                    NNDataIn[nexport].Pos[k]=Part[i].GetPosition(k);
                    NNDataIn[nexport].Vel[k]=Part[i].GetVelocity(k);
                }
                nexport++;
                nsend_local[cellnodeID]++;
                sent_mpi_domain[cellnodeID]++;
            }
        }
#ifdef STRUCDEN
    }
#endif
    }
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number
    if (nexport>0) qsort(NNDataIn, nexport, sizeof(struct nndata_in), nn_export_cmp);

    //then store the offset in the export particle data for the jth Task in order to send data.
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    //now send the data.
    ///\todo In determination of particle export, eventually need to place a check for the communication buffer so that if exported number
    ///is larger than the size of the buffer, iterate over the number exported
    //if either sending or receiving then run this process
    for (j=0;j<NProcs;j++)nimport+=mpi_nsend[ThisTask+j*NProcs];
    if (nexport>0||nimport>0) {
    for(j=0;j<NProcs;j++)//for(j=1;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;//ThisTask^j;
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                MPI_Sendrecv(&NNDataIn[noffset[recvTask]],
                    nsend_local[recvTask] * sizeof(struct nndata_in), MPI_BYTE,
                    recvTask, TAG_NN_A,
                    &NNDataGet[nbuffer[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct nndata_in),
                    MPI_BYTE, recvTask, TAG_NN_A, MPI_COMM_WORLD, &status);
            }
        }
    }
    }
    MPI_Barrier(MPI_COMM_WORLD);
}
#endif

/*! Mirror to \ref MPIGetNNExportNum, use exported particles, run ball search to find number of all local particles that need to be
    imported back to exported particle's thread so that a proper NN search can be made.
*/
void MPIGetNNImportNum(const Int_t nbodies, KDTree *tree, Particle *Part){
    Int_t i, j,nthreads,nexport=0,ncount;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Int_t oldnsend[NProcs*NProcs];
    Double_t xsearch[3][2];
    Int_t *nn=new Int_t[nbodies];
    Double_t *nnr2=new Double_t[nbodies];
    nthreads=1;
    Int_t sendTask,recvTask;
    MPI_Status status;
#ifdef USEOPENMP
#pragma omp parallel
    {
            if (omp_get_thread_num()==0) nthreads=omp_get_num_threads();
    }
#endif
    for(j=0;j<NProcs;j++)
    {
        nbuffer[j]=0;
        for (int k=0;k<j;k++)nbuffer[j]+=mpi_nsend[ThisTask+k*NProcs];//offset on "receiver" end
    }

    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (j=0;j<NProcs;j++) {
        for (i=0;i<nbodies;i++) nn[i]=-1;
        if (j!=ThisTask) {
            //search local list and tag all local particles that need to be exported back (or imported) to the exported particles thread
            for (i=nbuffer[j];i<nbuffer[j]+mpi_nsend[ThisTask+j*NProcs];i++) {
                tree->SearchBallPos(NNDataGet[i].Pos, NNDataGet[i].R2, j, nn, nnr2);
            }
            for (i=0;i<nbodies;i++) {
                if (nn[i]!=-1) {
                    for (int k=0;k<3;k++) {
                    }
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    //must store old mpi nsend for accessing NNDataGet properly.
    for (j=0;j<NProcs;j++) for (int k=0;k<NProcs;k++) oldnsend[k+j*NProcs]=mpi_nsend[k+j*NProcs];
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;
    for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    NExport=nexport;
    for (j=0;j<NProcs;j++) for (int k=0;k<NProcs;k++) mpi_nsend[k+j*NProcs]=oldnsend[k+j*NProcs];
}

/*! Mirror to \ref MPIBuildParticleNNExportList, use exported particles, run ball search to find all local particles that need to be
    imported back to exported particle's thread so that a proper NN search can be made.
*/
Int_t MPIBuildParticleNNImportList(const Int_t nbodies, KDTree *tree, Particle *Part, int iallflag){
    Int_t i, j,nthreads,nexport=0,ncount;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t *nn=new Int_t[nbodies];
    Double_t *nnr2=new Double_t[nbodies];
    nthreads=1;
    int sendTask,recvTask;
    int maxchunksize=2147483648/NProcs/sizeof(Particle);
    int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
    int sendoffset,recvoffset;
    int cursendchunksize,currecvchunksize;
    MPI_Status status;
#ifdef USEOPENMP
#pragma omp parallel
    {
            if (omp_get_thread_num()==0) nthreads=omp_get_num_threads();
    }
#endif
    for(j=0;j<NProcs;j++)
    {
        nbuffer[j]=0;
        for (int k=0;k<j;k++)nbuffer[j]+=mpi_nsend[ThisTask+k*NProcs];//offset on "receiver" end
    }

    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (j=0;j<NProcs;j++) {
#ifdef USEOPENMP
#pragma omp parallel default(shared) \
private(i)
{
#pragma omp for
#endif
            for (i=0;i<nbodies;i++) nn[i]=-1;
#ifdef USEOPENMP
}
#endif
        if (j!=ThisTask) {
            //search local list and tag all local particles that need to be exported back (or imported) to the exported particles thread
            for (i=nbuffer[j];i<nbuffer[j]+mpi_nsend[ThisTask+j*NProcs];i++) {
                tree->SearchBallPos(NNDataGet[i].Pos, NNDataGet[i].R2, j, nn, nnr2);
            }
            //if not spliting search so that only calculated velocity density function based on dark matter particles
            //as fof search is all but a separate baryon search is choosen for substructure, then just export particles
            //that are in spaitial window
            if (iallflag) {
            for (i=0;i<nbodies;i++) {
                if (nn[i]!=-1) {
                    for (int k=0;k<3;k++) {
                        PartDataIn[nexport].SetPosition(k,Part[i].GetPosition(k));
                        PartDataIn[nexport].SetVelocity(k,Part[i].GetVelocity(k));
                    }
                    nexport++;
                    nsend_local[j]++;
                }
            }
            }
            //otherwise, check the particle type either == dark matter or if struct den is on then type set to group number if dark and negative group number if not
            else {
            for (i=0;i<nbodies;i++) {
#ifdef STRUCDEN
                if (nn[i]!=-1 && Part[i].GetType()>0)
#else
                if (nn[i]!=-1 && Part[i].GetType()==DARKTYPE)
#endif
                {
                    for (int k=0;k<3;k++) {
                        PartDataIn[nexport].SetPosition(k,Part[i].GetPosition(k));
                        PartDataIn[nexport].SetVelocity(k,Part[i].GetVelocity(k));
                    }
                    nexport++;
                    nsend_local[j]++;
                }
            }
            }
        }
    }
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number
    //qsort(NNDataReturn, nexport, sizeof(struct nndata_in), nn_export_cmp);

    //then store the offset in the export particle data for the jth Task in order to send data.
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    //now send the data.
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer

            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //send info in loops to minimize memory footprint
                cursendchunksize=currecvchunksize=maxchunksize;
                nsendchunks=ceil(mpi_nsend[recvTask+ThisTask*NProcs]/(Double_t)maxchunksize);
                nrecvchunks=ceil(mpi_nsend[ThisTask+recvTask*NProcs]/(Double_t)maxchunksize);
                if (cursendchunksize>mpi_nsend[recvTask+ThisTask*NProcs]) {
                    nsendchunks=1;
                    cursendchunksize=mpi_nsend[recvTask+ThisTask*NProcs];
                }
                if (currecvchunksize>mpi_nsend[ThisTask+recvTask*NProcs]) {
                    nrecvchunks=1;
                    currecvchunksize=mpi_nsend[ThisTask+recvTask*NProcs];
                }
                numsendrecv=max(nsendchunks,nrecvchunks);
                sendoffset=recvoffset=0;
                for (auto ichunk=0;ichunk<numsendrecv;ichunk++)
                {
                    //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                    //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                    MPI_Sendrecv(&PartDataIn[noffset[recvTask]+sendoffset],
                        cursendchunksize * sizeof(Particle), MPI_BYTE,
                        recvTask, TAG_NN_B+ichunk,
                        &PartDataGet[nbuffer[recvTask]+recvoffset],
                        currecvchunksize * sizeof(Particle),
                        MPI_BYTE, recvTask, TAG_NN_B+ichunk, MPI_COMM_WORLD, &status);
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    if (cursendchunksize>mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset)cursendchunksize=mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset;
                    if (currecvchunksize>mpi_nsend[ThisTask+recvTask * NProcs]-sendoffset)currecvchunksize=mpi_nsend[ThisTask+recvTask * NProcs]-recvoffset;
                }
            }
        }
    }
    ncount=0;for (int k=0;k<NProcs;k++)ncount+=mpi_nsend[ThisTask+k*NProcs];
    return ncount;
}

/*! similar \ref MPIGetExportNum but number based on halo properties to see if any
*/
vector<bool> MPIGetHaloSearchExportNum(const Int_t ngroup, PropData *&pdata, vector<Double_t> &rdist)
{
    Int_t i,j,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;
    int indomain;
    vector<bool> halooverlap(ngroup+1);


    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=1;i<=ngroup;i++)
    {
        halooverlap[i]=false;
        for (int k=0;k<3;k++) {xsearch[k][0]=pdata[i].gcm[k]-rdist[i];xsearch[k][1]=pdata[i].gcm[k]+rdist[i];}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    nexport++;
                    nsend_local[j]++;
                    halooverlap[i]=true;
                }
            }
        }
    }
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;
    for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    NExport=nexport;
    return halooverlap;
}

/*! like \ref MPIBuildParticleExportList but each particle has a different distance stored in rdist used to find nearest neighbours
*/
void MPIBuildHaloSearchExportList(const Int_t ngroup, PropData *&pdata, vector<Double_t> &rdist, vector<bool> &halooverlap)
{
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    MPI_Status status;
    int indomain;
    int sendTask,recvTask;
    int maxchunksize=2147483648/NProcs/sizeof(nndata_in);
    int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
    int sendoffset,recvoffset;
    int cursendchunksize,currecvchunksize;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=1;i<=ngroup;i++)
    {
        if (halooverlap[i]==false) continue;
        for (int k=0;k<3;k++) {xsearch[k][0]=pdata[i].gcm[k]-rdist[i];xsearch[k][1]=pdata[i].gcm[k]+rdist[i];}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    //NNDataIn[nexport].Index=i;
                    NNDataIn[nexport].ToTask=j;
                    NNDataIn[nexport].FromTask=ThisTask;
                    NNDataIn[nexport].R2=rdist[i]*rdist[i];
                    //NNDataIn[nexport].V2=vdist2[i];
                    for (int k=0;k<3;k++) {
                        NNDataIn[nexport].Pos[k]=pdata[i].gcm[k];
                    }
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number
    if (nexport>0) qsort(NNDataIn, nexport, sizeof(struct nndata_in), nn_export_cmp);

    //then store the offset in the export data for the jth Task in order to send data.
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of items to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    //now send the data.

    for (j=0;j<NProcs;j++)nimport+=mpi_nsend[ThisTask+j*NProcs];
    //if task neither sends or receives, do nothing
    if (nexport==0&&nimport==0) return;
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //send info in loops to minimize memory footprint
                cursendchunksize=currecvchunksize=maxchunksize;
                nsendchunks=ceil(mpi_nsend[recvTask+ThisTask*NProcs]/(Double_t)maxchunksize);
                nrecvchunks=ceil(mpi_nsend[ThisTask+recvTask*NProcs]/(Double_t)maxchunksize);
                if (cursendchunksize>mpi_nsend[recvTask+ThisTask*NProcs]) {
                    nsendchunks=1;
                    cursendchunksize=mpi_nsend[recvTask+ThisTask*NProcs];
                }
                if (currecvchunksize>mpi_nsend[ThisTask+recvTask*NProcs]) {
                    nrecvchunks=1;
                    currecvchunksize=mpi_nsend[ThisTask+recvTask*NProcs];
                }
                numsendrecv=max(nsendchunks,nrecvchunks);
                sendoffset=recvoffset=0;
                for (auto ichunk=0;ichunk<numsendrecv;ichunk++)
                {
                    //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                    //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                    MPI_Sendrecv(&NNDataIn[noffset[recvTask]+sendoffset],
                        cursendchunksize * sizeof(struct nndata_in), MPI_BYTE,
                        recvTask, TAG_NN_A+ichunk,
                        &NNDataGet[nbuffer[recvTask]+recvoffset],
                        currecvchunksize * sizeof(struct nndata_in),
                        MPI_BYTE, recvTask, TAG_NN_A+ichunk, MPI_COMM_WORLD, &status);
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    if (cursendchunksize>mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset)cursendchunksize=mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset;
                    if (currecvchunksize>mpi_nsend[ThisTask+recvTask * NProcs]-sendoffset)currecvchunksize=mpi_nsend[ThisTask+recvTask * NProcs]-recvoffset;
                }
            }
        }
    }
}

/*! Mirror to \ref MPIGetHaloSearchExportNum, use exported positions, run ball search to find number of all local particles that need to be
    imported back to exported positions's thread so that a proper search can be made.
*/
void MPIGetHaloSearchImportNum(const Int_t nbodies, KDTree *tree, Particle *Part)
{
    Int_t i, j,nthreads,nexport=0,ncount;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Int_t oldnsend[NProcs*NProcs];
    Double_t xsearch[3][2];
    Int_t *nn=new Int_t[nbodies];
    Double_t *nnr2=new Double_t[nbodies];
    nthreads=1;
    Int_t sendTask,recvTask;
    MPI_Status status;
#ifdef USEOPENMP
#pragma omp parallel
    {
            if (omp_get_thread_num()==0) nthreads=omp_get_num_threads();
    }
#endif
    for(j=0;j<NProcs;j++)
    {
        nbuffer[j]=0;
        for (int k=0;k<j;k++)nbuffer[j]+=mpi_nsend[ThisTask+k*NProcs];//offset on "receiver" end
    }
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (j=0;j<NProcs;j++) {
        for (i=0;i<nbodies;i++) nn[i]=-1;
        if (j==ThisTask) continue;
        if (mpi_nsend[ThisTask+j*NProcs]==0) continue;
            //search local list and tag all local particles that need to be exported back (or imported) to the exported particles thread
            for (i=nbuffer[j];i<nbuffer[j]+mpi_nsend[ThisTask+j*NProcs];i++) {
                tree->SearchBallPos(NNDataGet[i].Pos, NNDataGet[i].R2, j, nn, nnr2);
            }
            for (i=0;i<nbodies;i++) {
                if (nn[i]!=-1) {
                    nexport++;
                    nsend_local[j]++;
                }
            }
        
    }
    //must store old mpi nsend for accessing NNDataGet properly.
    for (j=0;j<NProcs;j++) for (int k=0;k<NProcs;k++) oldnsend[k+j*NProcs]=mpi_nsend[k+j*NProcs];
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;
    for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    NExport=nexport;
    for (j=0;j<NProcs;j++) for (int k=0;k<NProcs;k++) mpi_nsend[k+j*NProcs]=oldnsend[k+j*NProcs];
}
/*! Mirror to \ref MPIBuildHaloSearchExportList, use exported particles, run ball search to find all local particles that need to be
    imported back to exported particle's thread so that a proper NN search can be made.
*/
Int_t MPIBuildHaloSearchImportList(const Int_t nbodies, KDTree *tree, Particle *Part){
    Int_t i, j,nthreads,nexport=0,ncount;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t *nn=new Int_t[nbodies];
    Double_t *nnr2=new Double_t[nbodies];
    nthreads=1;
    int sendTask,recvTask;
    int maxchunksize=2147483648/NProcs/sizeof(Particle);
    int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
    int sendoffset,recvoffset;
    int cursendchunksize,currecvchunksize;
    MPI_Status status;
#ifdef USEOPENMP
#pragma omp parallel
    {
            if (omp_get_thread_num()==0) nthreads=omp_get_num_threads();
    }
#endif
    for(j=0;j<NProcs;j++)
    {
        nbuffer[j]=0;
        for (int k=0;k<j;k++)nbuffer[j]+=mpi_nsend[ThisTask+k*NProcs];//offset on "receiver" end
    }

    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (j=0;j<NProcs;j++) {
            for (i=0;i<nbodies;i++) nn[i]=-1;
        if (j==ThisTask) continue;
        if (mpi_nsend[ThisTask+j*NProcs]==0) continue;
            //search local list and tag all local particles that need to be exported back (or imported) to the exported particles thread
            for (i=nbuffer[j];i<nbuffer[j]+mpi_nsend[ThisTask+j*NProcs];i++) {
                tree->SearchBallPos(NNDataGet[i].Pos, NNDataGet[i].R2, j, nn, nnr2);
            }
            for (i=0;i<nbodies;i++) {
                if (nn[i]!=-1) {
                    for (int k=0;k<3;k++) {
                        PartDataIn[nexport].SetPosition(k,Part[i].GetPosition(k));
                        PartDataIn[nexport].SetVelocity(k,Part[i].GetVelocity(k));
                    }
                    nexport++;
                    nsend_local[j]++;
                }
            }
    }
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number
    //qsort(NNDataReturn, nexport, sizeof(struct nndata_in), nn_export_cmp);

    //then store the offset in the export particle data for the jth Task in order to send data.
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    //now send the data.
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer

            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //send info in loops to minimize memory footprint
                cursendchunksize=currecvchunksize=maxchunksize;
                nsendchunks=ceil(mpi_nsend[recvTask+ThisTask*NProcs]/(Double_t)maxchunksize);
                nrecvchunks=ceil(mpi_nsend[ThisTask+recvTask*NProcs]/(Double_t)maxchunksize);
                if (cursendchunksize>mpi_nsend[recvTask+ThisTask*NProcs]) {
                    nsendchunks=1;
                    cursendchunksize=mpi_nsend[recvTask+ThisTask*NProcs];
                }
                if (currecvchunksize>mpi_nsend[ThisTask+recvTask*NProcs]) {
                    nrecvchunks=1;
                    currecvchunksize=mpi_nsend[ThisTask+recvTask*NProcs];
                }
                numsendrecv=max(nsendchunks,nrecvchunks);
                sendoffset=recvoffset=0;
                for (auto ichunk=0;ichunk<numsendrecv;ichunk++)
                {
                    //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                    //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                    MPI_Sendrecv(&PartDataIn[noffset[recvTask]+sendoffset],
                        cursendchunksize * sizeof(Particle), MPI_BYTE,
                        recvTask, TAG_NN_B+ichunk,
                        &PartDataGet[nbuffer[recvTask]+recvoffset],
                        currecvchunksize * sizeof(Particle),
                        MPI_BYTE, recvTask, TAG_NN_B+ichunk, MPI_COMM_WORLD, &status);
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    if (cursendchunksize>mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset)cursendchunksize=mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset;
                    if (currecvchunksize>mpi_nsend[ThisTask+recvTask * NProcs]-sendoffset)currecvchunksize=mpi_nsend[ThisTask+recvTask * NProcs]-recvoffset;
                }
            }
        }
    }
    ncount=0;for (int k=0;k<NProcs;k++)ncount+=mpi_nsend[ThisTask+k*NProcs];
    return ncount;
}


/*! Similar to \ref MPIBuildParticleExportList, however this is for associated baryon search where particles have been moved from original
    mpi domains and their group id accessed through the id array and their stored id and length in numingroup
*/
void MPIBuildParticleExportBaryonSearchList(const Int_t nbodies, Particle *Part, Int_t *&pfof, Int_t *ids, Int_t *numingroup, Double_t rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    int sendTask,recvTask;
    int maxchunksize=2147483648/NProcs/sizeof(fofdata_in);
    int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
    int sendoffset,recvoffset;
    int cursendchunksize,currecvchunksize;
    MPI_Status status;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++) {
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist;xsearch[k][1]=Part[i].GetPosition(k)+rdist;}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    //FoFDataIn[nexport].Part=Part[i];
                    FoFDataIn[nexport].Index = i;
                    FoFDataIn[nexport].Task = j;
                    FoFDataIn[nexport].iGroup = pfof[ids[Part[i].GetID()]];//set group id
                    FoFDataIn[nexport].iGroupTask = ThisTask;//and the task of the group
                    FoFDataIn[nexport].iLen = numingroup[pfof[ids[Part[i].GetID()]]];
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    if (nexport>0) {
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number
    qsort(FoFDataIn, nexport, sizeof(struct fofdata_in), fof_export_cmp);
    for (i=0;i<nexport;i++) PartDataIn[i] = Part[FoFDataIn[i].Index];
    }
    //then store the offset in the export particle data for the jth Task in order to send data.
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    //now send the data.
    ///\todo In determination of particle export for FOF routines, eventually need to place a check for the communication buffer so that if exported number
    ///is larger than the size of the buffer, iterate over the number exported
    for (j=0;j<NProcs;j++)nimport+=mpi_nsend[ThisTask+j*NProcs];
    if (nexport>0||nimport>0) {
        for(j=0;j<NProcs;j++)
        {
            if (j!=ThisTask)
            {
                sendTask = ThisTask;
                recvTask = j;
                nbuffer[recvTask]=0;
                for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer

                if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
                {
                    //send info in loops to minimize memory footprint
                    cursendchunksize=currecvchunksize=maxchunksize;
                    nsendchunks=ceil(mpi_nsend[recvTask+ThisTask*NProcs]/(Double_t)maxchunksize);
                    nrecvchunks=ceil(mpi_nsend[ThisTask+recvTask*NProcs]/(Double_t)maxchunksize);
                    if (cursendchunksize>mpi_nsend[recvTask+ThisTask*NProcs]) {
                        nsendchunks=1;
                        cursendchunksize=mpi_nsend[recvTask+ThisTask*NProcs];
                    }
                    if (currecvchunksize>mpi_nsend[ThisTask+recvTask*NProcs]) {
                        nrecvchunks=1;
                        currecvchunksize=mpi_nsend[ThisTask+recvTask*NProcs];
                    }
                    numsendrecv=max(nsendchunks,nrecvchunks);
                    sendoffset=recvoffset=0;
                    for (auto ichunk=0;ichunk<numsendrecv;ichunk++)
                    {
                        //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                        //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                        MPI_Sendrecv(&FoFDataIn[noffset[recvTask]+sendoffset],
                            cursendchunksize * sizeof(struct fofdata_in), MPI_BYTE,
                            recvTask, TAG_FOF_A+ichunk,
                            &FoFDataGet[nbuffer[recvTask]+recvoffset],
                            currecvchunksize * sizeof(struct fofdata_in),
                            MPI_BYTE, recvTask, TAG_FOF_A+ichunk, MPI_COMM_WORLD, &status);
                        MPI_Sendrecv(&PartDataIn[noffset[recvTask]+sendoffset],
                            cursendchunksize * sizeof(Particle), MPI_BYTE,
                            recvTask, TAG_FOF_B+ichunk,
                            &PartDataGet[nbuffer[recvTask]+recvoffset],
                            currecvchunksize * sizeof(Particle),
                            MPI_BYTE, recvTask, TAG_FOF_B+ichunk, MPI_COMM_WORLD, &status);
                        sendoffset+=cursendchunksize;
                        recvoffset+=currecvchunksize;
                        if (cursendchunksize>mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset)cursendchunksize=mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset;
                        if (currecvchunksize>mpi_nsend[ThisTask+recvTask * NProcs]-sendoffset)currecvchunksize=mpi_nsend[ThisTask+recvTask * NProcs]-recvoffset;
                    }
                }
            }
        }
    }
}

//@}

/// \name FOF related mpi routines
//@{
///Set fof task id of particle
short_mpi_t* MPISetTaskID(const Int_t nbodies){
    short_mpi_t *foftask=new short_mpi_t[nbodies];
    for (Int_t i=0;i<nbodies;i++) foftask[i]=ThisTask;
    return foftask;
}

///Offset pfof array based so that local group numbers do not overlap
///\todo alter to that this now ranks threads so that group ids are larger if thread has more particles. This ensures that mpi threads
///send particles to thread with the fewest particles when linking across mpi domains during a FOF search
void MPIAdjustLocalGroupIDs(const Int_t nbodies, Int_t *pfof){
    PriorityQueue *pq=new PriorityQueue(NProcs);
    for (int j=0;j<NProcs;j++) pq->Push(j,mpi_nlocal[j]);
    Int_t rankorder[NProcs];
    for (int j=0;j<NProcs;j++) {rankorder[NProcs-1-j]=pq->TopQueue();pq->Pop();}
    Int_t offset=0;
    for (int j=0;j<NProcs;j++){if(rankorder[j]==ThisTask) break; offset+=mpi_nlocal[rankorder[j]];}
    //Int_t offset=nbodies*ThisTask;
    for (Int_t i=0;i<nbodies;i++) if (pfof[i]>0) pfof[i]+=offset;
    mpi_maxgid=0;
    for (int j=0;j<NProcs;j++){mpi_maxgid+=mpi_nlocal[rankorder[j]];}
    mpi_gidoffset=0;
    for (int j=0;j<NProcs;j++){if(rankorder[j]==ThisTask) break; mpi_gidoffset+=mpi_ngroups[rankorder[j]];}
}

//My idea is this for doing the stiching. First generate export list of particle data and another seperate data structure for the FOF data
//Next, when examining local search using export particles (since iGroup=0 is unlinked) if a export particle current iGroup is larger
//Then adjust the local particle and all members of its group so long as its group is NOT group zero. Calculate the number of new links
//and determine the total number of new links across all mpi threads.
//If that number is not zero, then groups have been found that are across processor domains.
//One has to iterate the check across the domains till no more new links have been found. That is must update the export particles Group ids
//then begin the check anew.

//Couple of key things to think about are, one I really shouldn't have to run the check again to find the particles that meet the conditions across
//threads since that has NOT changed. must figure out a way to store relevant particles. Otherwise, continuously checking, seems a waste of cpu cycles.
//second I must pass along head, tail, next, and length information, maybe by using a plist structure so that it is easy to alter the particles locally to new group id
//Also must determine optimal way of setting which processor the group should end up on. Best way might be to use the length of the group locally since
//that would minimize the broadcasts.

/*! Particles that have been marked for export may have had their fof information updated so need to update this info
*/
void MPIUpdateExportList(const Int_t nbodies, Particle *Part, Int_t *&pfof, Int_tree_t *&Len){
    Int_t i, j,nthreads,nexport;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    int sendTask,recvTask;
    int maxchunksize=2147483648/NProcs/sizeof(fofdata_in);
    int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
    int sendoffset,recvoffset;
    int cursendchunksize,currecvchunksize;
    MPI_Status status;

    nexport=0;
    for (j=0;j<NProcs;j++) {nexport+=mpi_nsend[j+ThisTask*NProcs];nsend_local[j]=mpi_nsend[j+ThisTask*NProcs];}
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    for (i=0;i<nexport;i++) {
        FoFDataIn[i].iGroup = pfof[Part[FoFDataIn[i].Index].GetID()];
        FoFDataIn[i].iGroupTask=mpi_foftask[Part[FoFDataIn[i].Index].GetID()];
        FoFDataIn[i].iLen=Len[FoFDataIn[i].Index];
    }
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer

            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //send info in loops to minimize memory footprint
                cursendchunksize=currecvchunksize=maxchunksize;
                nsendchunks=ceil(mpi_nsend[recvTask+ThisTask*NProcs]/(Double_t)maxchunksize);
                nrecvchunks=ceil(mpi_nsend[ThisTask+recvTask*NProcs]/(Double_t)maxchunksize);
                if (cursendchunksize>mpi_nsend[recvTask+ThisTask*NProcs]) {
                    nsendchunks=1;
                    cursendchunksize=mpi_nsend[recvTask+ThisTask*NProcs];
                }
                if (currecvchunksize>mpi_nsend[ThisTask+recvTask*NProcs]) {
                    nrecvchunks=1;
                    currecvchunksize=mpi_nsend[ThisTask+recvTask*NProcs];
                }
                numsendrecv=max(nsendchunks,nrecvchunks);
                sendoffset=recvoffset=0;
                for (auto ichunk=0;ichunk<numsendrecv;ichunk++)
                {
                    //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer
                    //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                    MPI_Sendrecv(&FoFDataIn[noffset[recvTask]+sendoffset],
                        cursendchunksize * sizeof(struct fofdata_in), MPI_BYTE,
                        recvTask, TAG_FOF_A+ichunk,
                        &FoFDataGet[nbuffer[recvTask]+recvoffset],
                        currecvchunksize * sizeof(struct fofdata_in),
                        MPI_BYTE, recvTask, TAG_FOF_A+ichunk, MPI_COMM_WORLD, &status);
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    if (cursendchunksize>mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset)cursendchunksize=mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset;
                    if (currecvchunksize>mpi_nsend[ThisTask+recvTask * NProcs]-sendoffset)currecvchunksize=mpi_nsend[ThisTask+recvTask * NProcs]-recvoffset;
                }
            }
        }
    }
}

/*! This routine searches the local particle list using the positions of the exported particles to see if any local particles
    met the linking criterion and any other FOF criteria of said exported particle. If that is the case, then the group id of the local particle
    and all other particles that belong to the same group are adjusted if the group id of the exported particle is smaller. This routine returns
    the number of links found between the local particles and all other exported particles from all other mpi domains.
    \todo need to update lengths if strucden flag used to limit particles for which real velocity density calculated
*/
Int_t MPILinkAcross(const Int_t nbodies, KDTree *&tree, Particle *Part, Int_t *&pfof, Int_tree_t *&Len, Int_tree_t *&Head, Int_tree_t *&Next, Double_t rdist2){
    Int_t i,j,k;
    Int_t links=0;
    Int_t nbuffer[NProcs];
    Int_t *nn=new Int_t[nbodies];
    Int_t nt,ss,oldlen;
    Coordinate x;
    for (i=0;i<NImport;i++) {
        for (j=0;j<3;j++) x[j]=PartDataGet[i].GetPosition(j);
        //find all particles within a search radius of the imported particle
        nt=tree->SearchBallPosTagged(x, rdist2, nn);
        for (Int_t ii=0;ii<nt;ii++) {
            k=nn[ii];
            //if the imported particle does not belong to a group
            if (FoFDataGet[i].iGroup==0)
            {
                //if the current local particle's group head is zero and the exported particle group is zero
                //update the local particle's group id and the task to which it belongs
                //then one should link it and to make global decision base whether this task handles
                //the change on the PID of the particle
                if (pfof[Part[Head[k]].GetID()]==0&&Part[Head[k]].GetPID() > PartDataGet[i].GetPID()) {
                    pfof[Part[k].GetID()]=mpi_maxgid+mpi_gidoffset;///some unique identifier based on this task
                    mpi_gidoffset++;//increase unique identifier
                    Len[k]=1;
                    mpi_foftask[Part[k].GetID()]=FoFDataGet[i].iGroupTask;
                    links++;
                }
                //if the local particle does belong to a group, let the task from which the imported particle came from
                //handle the change
            }
            //if imported particle has already been linked
            else {
                //check to see if local particle has already been linked
                if (pfof[Part[Head[k]].GetID()]>0)  {
                    //as iGroups and pfof have been rank ordered globally
                    //proceed to link local particle to imported particle if
                    //its group id is larger
                    if(pfof[Part[Head[k]].GetID()] > FoFDataGet[i].iGroup) {
                        ss = Head[k];
                        oldlen=Len[k];
                        do{
                            pfof[Part[ss].GetID()]=FoFDataGet[i].iGroup;
                            mpi_foftask[Part[ss].GetID()]=FoFDataGet[i].iGroupTask;
                            Len[ss]=FoFDataGet[i].iLen+oldlen;
                        }while((ss = Next[ss]) >= 0);
                        FoFDataGet[i].iLen+=oldlen;
                        ss = Head[k];
                        links++;
                    }
                    //otherwise, let the task from which this imported particle came from
                    //handle the change
                }
                //if not in local group, add the particle to the imported particles group
                else {
                    pfof[Part[k].GetID()]=FoFDataGet[i].iGroup;
                    Len[k]=FoFDataGet[i].iLen;
                    mpi_foftask[Part[k].GetID()]=FoFDataGet[i].iGroupTask;
                    FoFDataGet[i].iLen+=1;
                    links++;
                }
            }
        }
    }
    return links;
}
///link particles belonging to the same group across mpi domains using comparison function
Int_t MPILinkAcross(const Int_t nbodies, KDTree *&tree, Particle *Part, Int_t *&pfof, Int_tree_t *&Len, Int_tree_t *&Head, Int_tree_t *&Next, Double_t rdist2, FOFcompfunc &cmp, Double_t *params){
    Int_t i,j,k;
    Int_t links=0;
    Int_t nbuffer[NProcs];
    Int_t *nn=new Int_t[nbodies];
    Int_t nt;
    for (i=0;i<NImport;i++) {
        nt=tree->SearchCriterionTagged(PartDataGet[i], cmp, params, nn);
        for (Int_t ii=0;ii<nt;ii++) {
            k=nn[ii];
            if (FoFDataGet[i].iGroup==0)
            {
                if (pfof[Part[Head[k]].GetID()]==0&&Part[Head[k]].GetPID() > PartDataGet[i].GetPID()) {
                    pfof[Part[k].GetID()]=mpi_maxgid+mpi_gidoffset;///some unique identifier based on this task
                    mpi_gidoffset++;//increase unique identifier
                    Len[k]=1;
                    mpi_foftask[Part[k].GetID()]=FoFDataGet[i].iGroupTask;
                    links++;
                }
            }
            else {
            if (pfof[Part[Head[k]].GetID()]>0)  {
                if(pfof[Part[Head[k]].GetID()] > FoFDataGet[i].iGroup) {
                    Int_t ss = Head[k];
                    do{
                        pfof[Part[ss].GetID()]=FoFDataGet[i].iGroup;
                        mpi_foftask[Part[ss].GetID()]=FoFDataGet[i].iGroupTask;
                        Len[ss]=FoFDataGet[i].iLen;
                    }while((ss = Next[ss]) >= 0);
                    ss = Head[k];
                    links++;
                }
            }
            else {
                pfof[Part[k].GetID()]=FoFDataGet[i].iGroup;
                Len[k]=FoFDataGet[i].iLen;
                mpi_foftask[Part[k].GetID()]=FoFDataGet[i].iGroupTask;
                links++;
            }
            }
        }
    }
    return links;
}

///link particles belonging to the same group across mpi domains given a type check function
Int_t MPILinkAcross(const Int_t nbodies, KDTree *&tree, Particle *Part, Int_t *&pfof, Int_tree_t *&Len, Int_tree_t *&Head, Int_tree_t *&Next, Double_t rdist2, FOFcheckfunc &check, Double_t *params){
    Int_t i,j,k;
    Int_t links=0;
    Int_t nbuffer[NProcs];
    Int_t *nn=new Int_t[nbodies];
    Int_t nt;
    bool iflag;
    Coordinate x;
    for (i=0;i<NImport;i++) {
        //if exported particle not in a group, do nothing
        if (FoFDataGet[i].iGroup==0) continue;
        for (j=0;j<3;j++) x[j]=PartDataGet[i].GetPosition(j);
        nt=tree->SearchBallPosTagged(x, rdist2, nn);
        for (Int_t ii=0;ii<nt;ii++) {
            k=nn[ii];
            //check that at least on of the particles meets the type criterion
            if (check(Part[k],params)!=0 && check(PartDataGet[i],params)!=0) continue;
            //if local particle in a group
            if (pfof[Part[Head[k]].GetID()]>0)  {
                //only change if both particles are appropriate type and group ids indicate local needs to be exported
                if (!(check(Part[k],params)==0 && check(PartDataGet[i],params)==0)) continue;
                if(pfof[Part[Head[k]].GetID()] > FoFDataGet[i].iGroup) {
                    Int_t ss = Head[k];
                    do{
                        pfof[Part[ss].GetID()]=FoFDataGet[i].iGroup;
                        mpi_foftask[Part[ss].GetID()]=FoFDataGet[i].iGroupTask;
                        Len[ss]=FoFDataGet[i].iLen;
                    }while((ss = Next[ss]) >= 0);
                    ss = Head[k];
                    links++;
                }
            }
            //if local particle not in a group and export is appropriate type, link
            else {
                if (check(PartDataGet[i],params)!=0) continue;
                pfof[Part[k].GetID()]=FoFDataGet[i].iGroup;
                Len[k]=FoFDataGet[i].iLen;
                mpi_foftask[Part[k].GetID()]=FoFDataGet[i].iGroupTask;
                links++;
            }
        }
    }
    return links;
}
/*!
    Group particles belong to a group to a particular mpi thread so that locally easy to determine
    the maximum group size and reoder the group ids according to descending group size.
    return the new local number of particles
*/
Int_t MPIGroupExchange(const Int_t nbodies, Particle *Part, Int_t *&pfof){
    Int_t i, j,nthreads,nexport,nimport,nlocal,n;
    Int_t nsend_local[NProcs],noffset_import[NProcs],noffset_export[NProcs],nbuffer[NProcs];
    int sendTask,recvTask;
    int maxchunksize=2147483648/NProcs/sizeof(fofid_in);
    int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
    int sendoffset,recvoffset;
    int cursendchunksize,currecvchunksize;
    MPI_Status status;

    int task;
    FoFGroupDataExport=NULL;
    FoFGroupDataLocal=NULL;
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    //first determine how big a local array is needed to store linked particles and broadcast information to create new nsend array
    nlocal=0;
    for (i=0;i<nbodies;i++) {
        if (mpi_foftask[i]!=ThisTask)
            nsend_local[mpi_foftask[i]]++;
    }
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    nexport=nimport=0;
    for (j=0;j<NProcs;j++){
        nimport+=mpi_nsend[ThisTask+j*NProcs];
        nexport+=mpi_nsend[j+ThisTask*NProcs];
    }
    //declare array for local storage of the appropriate size
    nlocal=nbodies-nexport+nimport;
    NImport=nimport;
    if (nexport >0) FoFGroupDataExport=new fofid_in[nexport];
    else FoFGroupDataExport=new fofid_in[1];

    Int_t *storeval=new Int_t[nbodies];
    Noldlocal=nbodies-nexport;
    //for (i=0;i<nbodies;i++) Part[i].SetID(i);
    //store type in temporary array, then use type to store what task particle belongs to and sort values
    for (i=0;i<nbodies;i++) storeval[i]=Part[i].GetType();
    for (i=0;i<nbodies;i++) Part[i].SetType((mpi_foftask[i]!=ThisTask));
    qsort(Part,nbodies,sizeof(Particle),TypeCompare);
    //sort(Part.begin(),Part.begin()+nbodies,TypeCompareVec);
    for (i=0;i<nbodies;i++) Part[i].SetType(storeval[Part[i].GetID()]);
    //now use array to rearrange data
    for (i=0;i<nbodies;i++) storeval[i]=mpi_foftask[Part[i].GetID()];
    for (i=0;i<nbodies;i++) mpi_foftask[i]=storeval[i];
    for (i=0;i<nbodies;i++) storeval[i]=pfof[Part[i].GetID()];
    for (i=0;i<nbodies;i++) pfof[i]=storeval[i];
    for (i=0;i<nbodies;i++) Part[i].SetID(i);
    //for sorting purposes to place untagged particles at the end. Was done by setting type
    //now via storeval and ids
    for (i=0;i<nbodies;i++) storeval[i]=-pfof[Part[i].GetID()];
    for (i=0;i<nbodies;i++) Part[i].SetID(storeval[i]);
    if (nimport>0) FoFGroupDataLocal=new fofid_in[nimport];
    delete[] storeval;

    //determine offsets in arrays so that data contiguous with regards to processors for broadcasting
    //offset on transmitter end
    noffset_export[0]=0;
    for (j=1;j<NProcs;j++) noffset_export[j]=noffset_export[j-1]+mpi_nsend[(j-1)+ThisTask*NProcs];
    //offset on receiver end
    for (j=0;j<NProcs;j++) {
        noffset_import[j]=0;
        if (j!=ThisTask) for (int k=0;k<j;k++)noffset_import[j]+=mpi_nsend[ThisTask+k*NProcs];
    }
    for (j=0;j<NProcs;j++) nbuffer[j]=0;
    for (i=nbodies-nexport;i<nbodies;i++) {
        //if particle belongs to group that should belong on a different mpi thread, store for broadcasting
        task=mpi_foftask[i];
        if (task!=ThisTask) {
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].p=Part[i];
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Index = i;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Task = task;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].iGroup = pfof[i];
        }
        nbuffer[task]++;
    }
    //now send the data.
    ///\todo In determination of particle export for FOF routines, eventually need to place a check for the communication buffer so that if exported number
    ///is larger than the size of the buffer, iterate over the number exported
    ///\todo ideally need to implement MPI Type defs and serialization using boost
    //unsigned int chunksize,chunksize_send,chunksize_recv,nchunks,nchunks_send,nchunks_recv;
    //unsigned long offset_send,offset_recv;
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;

            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //send info in loops to minimize memory footprint
                cursendchunksize=currecvchunksize=maxchunksize;
                nsendchunks=ceil(mpi_nsend[recvTask+ThisTask*NProcs]/(Double_t)maxchunksize);
                nrecvchunks=ceil(mpi_nsend[ThisTask+recvTask*NProcs]/(Double_t)maxchunksize);
                if (cursendchunksize>mpi_nsend[recvTask+ThisTask*NProcs]) {
                    nsendchunks=1;
                    cursendchunksize=mpi_nsend[recvTask+ThisTask*NProcs];
                }
                if (currecvchunksize>mpi_nsend[ThisTask+recvTask*NProcs]) {
                    nrecvchunks=1;
                    currecvchunksize=mpi_nsend[ThisTask+recvTask*NProcs];
                }
                numsendrecv=max(nsendchunks,nrecvchunks);
                sendoffset=recvoffset=0;
                for (auto ichunk=0;ichunk<numsendrecv;ichunk++)
                {
                    MPI_Sendrecv(&FoFGroupDataExport[noffset_export[recvTask]+sendoffset],
                        cursendchunksize * sizeof(struct fofid_in), MPI_BYTE,
                        recvTask, TAG_FOF_C+ichunk,
                        &FoFGroupDataLocal[noffset_import[recvTask]+recvoffset],
                        currecvchunksize * sizeof(struct fofid_in),
                        MPI_BYTE, recvTask, TAG_FOF_C+ichunk, MPI_COMM_WORLD, &status);
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    if (cursendchunksize>mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset)cursendchunksize=mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset;
                    if (currecvchunksize>mpi_nsend[ThisTask+recvTask * NProcs]-sendoffset)currecvchunksize=mpi_nsend[ThisTask+recvTask * NProcs]-recvoffset;
                }
            }
        }
    }
    Nlocal=nlocal;
    return nlocal;
}

/*!
    The baryon equivalent of \ref MPIGroupExchange. Here assume baryons are searched afterwards
*/
Int_t MPIBaryonGroupExchange(const Int_t nbodies, Particle *Part, Int_t *&pfof){
    Int_t i, j,nthreads,nexport,nimport,nlocal,n;
    Int_t nsend_local[NProcs],noffset_import[NProcs],noffset_export[NProcs],nbuffer[NProcs];
    int sendTask,recvTask;
    int maxchunksize=2147483648/NProcs/sizeof(fofid_in);
    int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
    int sendoffset,recvoffset;
    int cursendchunksize,currecvchunksize;
    MPI_Status status;
    int task;
    FoFGroupDataExport=NULL;
    FoFGroupDataLocal=NULL;
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    //first determine how big a local array is needed to store linked particles and broadcast information to create new nsend array
    nlocal=0;
    for (i=0;i<nbodies;i++) {
        if (mpi_foftask[i]!=ThisTask)
            nsend_local[mpi_foftask[i]]++;
    }
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    nexport=nimport=0;
    for (j=0;j<NProcs;j++){
        nimport+=mpi_nsend[ThisTask+j*NProcs];
        nexport+=mpi_nsend[j+ThisTask*NProcs];
    }
    //declare array for local storage of the appropriate size
    nlocal=nbodies-nexport+nimport;
    NImport=nimport;
    if (nexport >0) FoFGroupDataExport=new fofid_in[nexport];
    else FoFGroupDataExport=new fofid_in[1];

    Nmemlocalbaryon=Nlocalbaryon[0];
    for (i=0;i<nbodies;i++) Part[i].SetID(i);
    Int_t *storeval=new Int_t[nbodies];
    //if trying to reduce memory allocation,  if nlocal < than the memory allocated adjust local list so that all particles to be exported are near the end.
    //and allocate the appropriate memory for pfof and mpi_idlist. otherwise, need to actually copy the particle data into FoFGroupDataLocal and proceed
    //as normal, storing info, send info, delete particle array, allocate a new array large enough to store info and copy over info
    ///\todo eventually I should replace arrays with vectors so that the size can change, removing the need to free and allocate
    if (nlocal<=Nmemlocalbaryon) {
        Noldlocal=nbodies-nexport;
        for (i=0;i<nbodies;i++) storeval[i]=Part[i].GetType();
        for (i=0;i<nbodies;i++) Part[i].SetType((mpi_foftask[i]!=ThisTask));
        qsort(Part,nbodies,sizeof(Particle),TypeCompare);
        for (i=0;i<nbodies;i++) Part[i].SetType(storeval[Part[i].GetID()]);
        //now use array to rearrange data
        for (i=0;i<nbodies;i++) storeval[i]=mpi_foftask[Part[i].GetID()];
        for (i=0;i<nbodies;i++) mpi_foftask[i]=storeval[i];
        for (i=0;i<nbodies;i++) storeval[i]=pfof[Part[i].GetID()];
        for (i=0;i<nbodies;i++) pfof[i]=storeval[i];
        for (i=0;i<nbodies;i++) Part[i].SetID(i);
        //now via storeval and ids
        for (i=0;i<nbodies;i++) storeval[i]=-pfof[Part[i].GetID()];
        for (i=0;i<nbodies;i++) Part[i].SetID(storeval[i]);
        if (nimport>0) FoFGroupDataLocal=new fofid_in[nimport];
    }
    //otherwise use FoFGroupDataLocal to store all the necessary data
    else {
        FoFGroupDataLocal=new fofid_in[nlocal];
        for (i=0;i<nbodies;i++) storeval[i]=Part[i].GetType();
        for (i=0;i<nbodies;i++) Part[i].SetType((mpi_foftask[i]!=ThisTask));
        qsort(Part,nbodies,sizeof(Particle),TypeCompare);
        for (i=0;i<nbodies;i++) Part[i].SetType(storeval[Part[i].GetID()]);
        Int_t nn=nbodies-nexport;
        for (i=0;i<nn;i++) {
            FoFGroupDataLocal[i].p=Part[i];
            FoFGroupDataLocal[i].Index = i;
            FoFGroupDataLocal[i].Task = ThisTask;
            FoFGroupDataLocal[i].iGroup = pfof[Part[i].GetID()];
        }
        for (i=nn;i<nbodies;i++) storeval[i]=mpi_foftask[Part[i].GetID()];
        for (i=nn;i<nbodies;i++) mpi_foftask[i]=storeval[i];
        for (i=nn;i<nbodies;i++) storeval[i]=pfof[Part[i].GetID()];
        for (i=nn;i<nbodies;i++) pfof[i]=storeval[i];
        for (i=nn;i<nbodies;i++) Part[i].SetID(i);
    }
    delete[] storeval;
    //determine offsets in arrays so that data contiguous with regards to processors for broadcasting
    //offset on transmitter end
    noffset_export[0]=0;
    for (j=1;j<NProcs;j++) noffset_export[j]=noffset_export[j-1]+mpi_nsend[(j-1)+ThisTask*NProcs];
    //offset on receiver end
    for (j=0;j<NProcs;j++) {
        if (nlocal<Nlocalbaryon[0]) noffset_import[j]=0;
        else noffset_import[j]=nbodies-nexport;
        if (j!=ThisTask) for (int k=0;k<j;k++)noffset_import[j]+=mpi_nsend[ThisTask+k*NProcs];
    }
    for (j=0;j<NProcs;j++) nbuffer[j]=0;
    for (i=nbodies-nexport;i<nbodies;i++) {
        //if particle belongs to group that should belong on a different mpi thread, store for broadcasting
        task=mpi_foftask[i];
        if (task!=ThisTask) {
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].p=Part[i];
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Index = i;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Task = task;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].iGroup = pfof[i];
        }
        nbuffer[task]++;
    }
    //now send the data.
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;

            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //send info in loops to minimize memory footprint
                cursendchunksize=currecvchunksize=maxchunksize;
                nsendchunks=ceil(mpi_nsend[recvTask+ThisTask*NProcs]/(Double_t)maxchunksize);
                nrecvchunks=ceil(mpi_nsend[ThisTask+recvTask*NProcs]/(Double_t)maxchunksize);
                if (cursendchunksize>mpi_nsend[recvTask+ThisTask*NProcs]) {
                    nsendchunks=1;
                    cursendchunksize=mpi_nsend[recvTask+ThisTask*NProcs];
                }
                if (currecvchunksize>mpi_nsend[ThisTask+recvTask*NProcs]) {
                    nrecvchunks=1;
                    currecvchunksize=mpi_nsend[ThisTask+recvTask*NProcs];
                }
                numsendrecv=max(nsendchunks,nrecvchunks);
                sendoffset=recvoffset=0;
                for (auto ichunk=0;ichunk<numsendrecv;ichunk++)
                {
                    MPI_Sendrecv(&FoFGroupDataExport[noffset_export[recvTask]+sendoffset],
                        cursendchunksize * sizeof(struct fofid_in), MPI_BYTE,
                        recvTask, TAG_FOF_C+ichunk,
                        &FoFGroupDataLocal[noffset_import[recvTask]+recvoffset],
                        currecvchunksize * sizeof(struct fofid_in),
                        MPI_BYTE, recvTask, TAG_FOF_C+ichunk, MPI_COMM_WORLD, &status);
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    if (cursendchunksize>mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset)cursendchunksize=mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset;
                    if (currecvchunksize>mpi_nsend[ThisTask+recvTask * NProcs]-sendoffset)currecvchunksize=mpi_nsend[ThisTask+recvTask * NProcs]-recvoffset;
                }
            }
        }
    }
    Nlocalbaryon[0]=nlocal;
    return nlocal;
}

///Determine the local number of groups and their sizes (groups must be local to an mpi thread)
Int_t MPICompileGroups(const Int_t nbodies, Particle *Part, Int_t *&pfof, Int_t minsize){
    Int_t i,j,start,ngroups;
    Int_t *numingroup,*groupid,**plist;
    ngroups=0;
    for (i=Noldlocal;i<nbodies;i++) {
        Part[i]=FoFGroupDataLocal[i-Noldlocal].p;
        //note that before used type to sort particles
        //now use id
        Part[i].SetID(-FoFGroupDataLocal[i-Noldlocal].iGroup);
    }
    //used to use ID store store group id info
    qsort(Part,nbodies,sizeof(Particle),IDCompare);
    //determine the # of groups, their size and the current group ID
    for (i=0,start=0;i<nbodies;i++) {
        if (Part[i].GetID()!=Part[start].GetID()) {
            //if group is too small set type to zero, which currently is used to store the group id
            if ((i-start)<minsize) for (Int_t j=start;j<i;j++) Part[j].SetID(0);
            else ngroups++;
            start=i;
        }
        if (Part[i].GetID()==0) break;
    }
    //again resort to move untagged particles to the end.
    qsort(Part,nbodies,sizeof(Particle),IDCompare);
    //now adjust pfof and ids.
    for (i=0;i<nbodies;i++) {pfof[i]=-Part[i].GetID();Part[i].SetID(i);}
    numingroup=new Int_t[ngroups+1];
    plist=new Int_t*[ngroups+1];
    ngroups=1;//offset as group zero is untagged
    for (i=0,start=0;i<nbodies;i++) {
        if (pfof[i]!=pfof[start]) {
            numingroup[ngroups]=i-start;
            plist[ngroups]=new Int_t[numingroup[ngroups]];
            for (Int_t j=start,count=0;j<i;j++) plist[ngroups][count++]=j;
            ngroups++;
            start=i;
        }
        if (pfof[i]==0) break;
    }
    ngroups--;

    //reorder groups ids according to size
    ReorderGroupIDs(ngroups,ngroups,numingroup,pfof,plist);
    for (i=1;i<=ngroups;i++) delete[] plist[i];
    delete[] plist;
    delete[] numingroup;
    //broadcast number of groups so that ids can be properly offset
    MPI_Allgather(&ngroups, 1, MPI_Int_t, mpi_ngroups, 1, MPI_Int_t, MPI_COMM_WORLD);
    if(FoFGroupDataLocal!=NULL) delete[] FoFGroupDataLocal;
    if(FoFGroupDataExport!=NULL) delete[] FoFGroupDataExport;
    return ngroups;
}

///Similar to \ref MPICompileGroups but optimised for separate baryon search
///\todo need to update to reflect vector implementation
Int_t MPIBaryonCompileGroups(const Int_t nbodies, Particle *Part, Int_t *&pfof, Int_t minsize, int iorder){
    Int_t i,j,start,ngroups;
    Int_t *numingroup,*groupid,**plist;
    ngroups=0;

    //if minimizing memory load when using mpi (by adding extra routines to determine memory required)
    //first check to see if local memory is enough to contained expected number of particles
    //if local mem is enough, copy data from the FoFGroupDataLocal
    if(Nmemlocalbaryon>nbodies) {
        for (i=Noldlocal;i<nbodies;i++) {
            Part[i]=FoFGroupDataLocal[i-Noldlocal].p;
            Part[i].SetID(-FoFGroupDataLocal[i-Noldlocal].iGroup);
        }
        //now use ID
        qsort(Part,nbodies,sizeof(Particle),IDCompare);
        //determine the # of groups, their size and the current group ID
        for (i=0,start=0;i<nbodies;i++) {
            if (Part[i].GetID()!=Part[start].GetID()) {
                //if group is too small set type to zero, which currently is used to store the group id
                if ((i-start)<minsize) for (Int_t j=start;j<i;j++) Part[j].SetID(0);
                else ngroups++;
                start=i;
            }
            if (Part[i].GetID()==0) break;
        }

        //again resort to move untagged particles to the end.
        qsort(Part,nbodies,sizeof(Particle),IDCompare);
        //now adjust pfof and ids.
        for (i=0;i<nbodies;i++) {pfof[i]=-Part[i].GetID();Part[i].SetID(i);}
        numingroup=new Int_t[ngroups+1];
        plist=new Int_t*[ngroups+1];
        ngroups=1;//offset as group zero is untagged
        for (i=0,start=0;i<nbodies;i++) {
            if (pfof[i]!=pfof[start]) {
                numingroup[ngroups]=i-start;
                plist[ngroups]=new Int_t[numingroup[ngroups]];
                for (Int_t j=start,count=0;j<i;j++) plist[ngroups][count++]=j;
                ngroups++;
                start=i;
            }
            if (pfof[i]==0) break;
        }
        ngroups--;
    }
    else {
        //sort local list
        qsort(FoFGroupDataLocal, nbodies, sizeof(struct fofid_in), fof_id_cmp);
        //determine the # of groups, their size and the current group ID
        for (i=0,start=0;i<nbodies;i++) {
            if (FoFGroupDataLocal[i].iGroup!=FoFGroupDataLocal[start].iGroup) {
                if ((i-start)<minsize){
                    for (Int_t j=start;j<i;j++) FoFGroupDataLocal[j].iGroup=0;
                }
                else ngroups++;
                start=i;
            }
            if (FoFGroupDataLocal[i].iGroup==0) break;
        }
        //now sort again which will put particles group then id order, and determine size of groups and their current group id;
        qsort(FoFGroupDataLocal, nbodies, sizeof(struct fofid_in), fof_id_cmp);
        numingroup=new Int_t[ngroups+1];
        plist=new Int_t*[ngroups+1];
        ngroups=1;//offset as group zero is untagged
        for (i=0,start=0;i<nbodies;i++) {
            if (FoFGroupDataLocal[i].iGroup!=FoFGroupDataLocal[start].iGroup) {
                numingroup[ngroups]=i-start;
                plist[ngroups]=new Int_t[numingroup[ngroups]];
                for (Int_t j=start,count=0;j<i;j++) plist[ngroups][count++]=j;
                ngroups++;
                start=i;
            }
            if (FoFGroupDataLocal[i].iGroup==0) break;
        }
        ngroups--;
        for (i=0;i<nbodies;i++) pfof[i]=FoFGroupDataLocal[i].iGroup;
        //and store the particles global ids
        for (i=0;i<nbodies;i++) {
            Part[i]=FoFGroupDataLocal[i].p;
            Part[i].SetID(i);
        }
    }
    //reorder groups ids according to size if required.
    if (iorder) ReorderGroupIDs(ngroups,ngroups,numingroup,pfof,plist);
    for (i=1;i<=ngroups;i++) if (numingroup[i]>0) delete[] plist[i];
    delete[] plist;
    delete[] numingroup;

    //broadcast number of groups so that ids can be properly offset
    MPI_Allgather(&ngroups, 1, MPI_Int_t, mpi_ngroups, 1, MPI_Int_t, MPI_COMM_WORLD);
    if(FoFGroupDataLocal!=NULL) delete[] FoFGroupDataLocal;
    if(FoFGroupDataExport!=NULL) delete[] FoFGroupDataExport;
    return ngroups;
}

///Determine which exported dm particle is closest in phase-space to a local baryon particle and assign that particle to the group of that dark matter particle if is closest particle
Int_t MPISearchBaryons(const Int_t nbaryons, Particle *&Pbaryons, Int_t *&pfofbaryons, Int_t *numingroup, Double_t *localdist, Int_t nsearch, Double_t *param, Double_t *period)
{
    Double_t D2, dval, rval;
    Coordinate x1;
    Particle p1;
    Int_t  i, j, k, pindex,nexport=0;
    int tid;
    FOFcompfunc fofcmp=FOF6d;
    Int_t *nnID;
    Double_t *dist2;
    if (NImport>0) {
    //now dark matter particles associated with a group existing on another mpi domain are local and can be searched.
    KDTree *mpitree=new KDTree(PartDataGet,NImport,nsearch/2,mpitree->TPHYS,mpitree->KEPAN,100,0,0,0,period);
    if (nsearch>NImport) nsearch=NImport;
#ifdef USEOPENMP
#pragma omp parallel default(shared) \
private(i,j,k,tid,p1,pindex,x1,D2,dval,rval,nnID,dist2)
{
    nnID=new Int_t[nsearch];
    dist2=new Double_t[nsearch];
#pragma omp for reduction(+:nexport)
#endif
    for (i=0;i<nbaryons;i++)
    {
#ifdef USEOPENMP
        tid=omp_get_thread_num();
#else
        tid=0;
#endif
        p1=Pbaryons[i];
        x1=Coordinate(p1.GetPosition());
        rval=MAXVALUE;
        dval=localdist[i];
        mpitree->FindNearestPos(x1, nnID, dist2,nsearch);
        if (dist2[0]<param[6]) {
        for (j=0;j<nsearch;j++) {
            D2=0;
            pindex=PartDataGet[nnID[j]].GetID();
            if (numingroup[pfofbaryons[i]]<FoFDataGet[pindex].iLen) {
                if (fofcmp(p1,PartDataGet[nnID[j]],param)) {
                    for (k=0;k<3;k++) {
                        D2+=(p1.GetPosition(k)-PartDataGet[nnID[j]].GetPosition(k))*(p1.GetPosition(k)-PartDataGet[nnID[j]].GetPosition(k))/param[6]+(p1.GetVelocity(k)-PartDataGet[nnID[j]].GetVelocity(k))*(p1.GetVelocity(k)-PartDataGet[nnID[j]].GetVelocity(k))/param[7];
                    }
#ifdef GASON
                    D2+=p1.GetU()/param[7];
#endif
                    if (dval>D2) {dval=D2;pfofbaryons[i]=FoFDataGet[pindex].iGroup;rval=dist2[j];mpi_foftask[i]=FoFDataGet[pindex].iGroupTask;}
                }
            }
        }
        }
        nexport+=(mpi_foftask[i]!=ThisTask);
    }
    delete[] nnID;
    delete[] dist2;
#ifdef USEOPENMP
}
#endif
    }
    return nexport;
}

Int_t MPIBaryonExchange(const Int_t nbaryons, Particle *Pbaryons, Int_t *pfofbaryons){
    Int_t i, j,nthreads,nexport,nimport,nlocal,n;
    Int_t nsend_local[NProcs],noffset_import[NProcs],noffset_export[NProcs],nbuffer[NProcs];
    int sendTask,recvTask;
    int maxchunksize=2147483648/NProcs/sizeof(fofid_in);
    int nsend,nrecv,nsendchunks,nrecvchunks,numsendrecv;
    int sendoffset,recvoffset;
    int cursendchunksize,currecvchunksize;
    MPI_Status status;
    int task;
    //initial containers to send info across threads
    FoFGroupDataExport=NULL;
    FoFGroupDataLocal=NULL;

    MPI_Barrier(MPI_COMM_WORLD);
    //first determine how big a local array is needed to store tagged baryonic particles
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    nlocal=0;
    for (i=0;i<nbaryons;i++) {
        if (mpi_foftask[i]!=ThisTask)
            nsend_local[mpi_foftask[i]]++;
    }
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    nexport=nimport=0;
    for (j=0;j<NProcs;j++){
        nimport+=mpi_nsend[ThisTask+j*NProcs];
        nexport+=mpi_nsend[j+ThisTask*NProcs];
    }
    //declare array for local storage of the appropriate size
    nlocal=nbaryons-nexport+nimport;
    //store import number
    NImport=nimport;
    FoFGroupDataExport=new fofid_in[nexport+1];//+1 just buffer to ensure that if nothing broadcast, easy to allocate and deallocate memory

    //if trying to reduce memory allocation, then need to check amount stored locally and how much that needs to be adjusted by
    //if nlocal < than the memory allocated adjust local list so that all particles to be exported are near the end.
    //and allocate the appropriate memory for pfofbaryons and mpi_idlist. otherwise, need to actually copy the particle data into FoFGroupDataLocal and proceed
    //as normal, storing info, send info, delete particle array, allocate a new array large enough to store info and copy over info
    ///\todo eventually I should replace arrays with vectors so that the size can change, removing the need to free and allocate
    Int_t *storeval=new Int_t[nbaryons];
    if (nlocal<Nmemlocal) {
        Noldlocal=nbaryons-nexport;
        for (i=0;i<nbaryons;i++) storeval[i]=Pbaryons[i].GetType();
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetType((mpi_foftask[i]!=ThisTask));
        qsort(Pbaryons,nbaryons,sizeof(Particle),TypeCompare);
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetType(storeval[Pbaryons[i].GetID()]);
        //now use array to rearrange data
        for (i=0;i<nbaryons;i++) storeval[i]=mpi_foftask[Pbaryons[i].GetID()];
        for (i=0;i<nbaryons;i++) mpi_foftask[i]=storeval[i];
        for (i=0;i<nbaryons;i++) storeval[i]=pfofbaryons[Pbaryons[i].GetID()];
        for (i=0;i<nbaryons;i++) pfofbaryons[i]=storeval[i];
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetID(i);
        //for sorting purposes to place untagged particles at the end.
        for (i=0;i<nbaryons;i++) storeval[i]=-pfofbaryons[Pbaryons[i].GetID()];
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetID(storeval[i]);
        if (nimport>0) FoFGroupDataLocal=new fofid_in[nimport];
    }
    //otherwise use FoFGroupDataLocal to store all the necessary data
    else {
        FoFGroupDataLocal=new fofid_in[nlocal];
        for (i=0;i<nbaryons;i++) storeval[i]=Pbaryons[i].GetType();
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetType((mpi_foftask[i]!=ThisTask));
        qsort(Pbaryons,nbaryons,sizeof(Particle),TypeCompare);
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetType(storeval[Pbaryons[i].GetID()]);
        Int_t nn=nbaryons-nexport;
        for (i=0;i<nn;i++) {
            FoFGroupDataLocal[i].p=Pbaryons[i];
            FoFGroupDataLocal[i].Index = i;
            FoFGroupDataLocal[i].Task = ThisTask;
            FoFGroupDataLocal[i].iGroup = pfofbaryons[Pbaryons[i].GetID()];
        }
        for (i=nn;i<nbaryons;i++) storeval[i]=mpi_foftask[Pbaryons[i].GetID()];
        for (i=nn;i<nbaryons;i++) mpi_foftask[i]=storeval[i];
        for (i=nn;i<nbaryons;i++) storeval[i]=pfofbaryons[Pbaryons[i].GetID()];
        for (i=nn;i<nbaryons;i++) pfofbaryons[i]=storeval[i];
        for (i=nn;i<nbaryons;i++) Pbaryons[i].SetID(i);
    }
    delete[] storeval;

    //determine offsets in arrays so that data contiguous with regards to processors for broadcasting
    //offset on transmitter end
    noffset_export[0]=0;
    for (j=1;j<NProcs;j++) noffset_export[j]=noffset_export[j-1]+mpi_nsend[(j-1)+ThisTask*NProcs];
    for (j=0;j<NProcs;j++) {
        if (nlocal<Nmemlocal) noffset_import[j]=0;
        else noffset_import[j]=nbaryons-nexport;
        if (j!=ThisTask) for (int k=0;k<j;k++)noffset_import[j]+=mpi_nsend[ThisTask+k*NProcs];
    }
    for (j=0;j<NProcs;j++) nbuffer[j]=0;
    for (i=nbaryons-nexport;i<nbaryons;i++) {
        //if particle belongs to group that should belong on a different mpi thread, store for broadcasting
        task=mpi_foftask[i];
        if (task!=ThisTask) {
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].p=Pbaryons[i];
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Index = i;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Task = task;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].iGroup = pfofbaryons[i];
        }
        nbuffer[task]++;
    }
    //now send the data.
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;

            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //send info in loops to minimize memory footprint
                cursendchunksize=currecvchunksize=maxchunksize;
                nsendchunks=ceil(mpi_nsend[recvTask+ThisTask*NProcs]/(Double_t)maxchunksize);
                nrecvchunks=ceil(mpi_nsend[ThisTask+recvTask*NProcs]/(Double_t)maxchunksize);
                if (cursendchunksize>mpi_nsend[recvTask+ThisTask*NProcs]) {
                    nsendchunks=1;
                    cursendchunksize=mpi_nsend[recvTask+ThisTask*NProcs];
                }
                if (currecvchunksize>mpi_nsend[ThisTask+recvTask*NProcs]) {
                    nrecvchunks=1;
                    currecvchunksize=mpi_nsend[ThisTask+recvTask*NProcs];
                }
                numsendrecv=max(nsendchunks,nrecvchunks);
                sendoffset=recvoffset=0;
                for (auto ichunk=0;ichunk<numsendrecv;ichunk++)
                {
                    MPI_Sendrecv(&FoFGroupDataExport[noffset_export[recvTask]+sendoffset],
                        cursendchunksize * sizeof(struct fofid_in), MPI_BYTE,
                        recvTask, TAG_FOF_C+ichunk,
                        &FoFGroupDataLocal[noffset_import[recvTask]+recvoffset],
                        currecvchunksize * sizeof(struct fofid_in),
                        MPI_BYTE, recvTask, TAG_FOF_C+ichunk, MPI_COMM_WORLD, &status);
                    sendoffset+=cursendchunksize;
                    recvoffset+=currecvchunksize;
                    if (cursendchunksize>mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset)cursendchunksize=mpi_nsend[recvTask+ThisTask * NProcs]-sendoffset;
                    if (currecvchunksize>mpi_nsend[ThisTask+recvTask * NProcs]-sendoffset)currecvchunksize=mpi_nsend[ThisTask+recvTask * NProcs]-recvoffset;
                }
            }
        }
    }
    Nlocalbaryon[0]=nlocal;
    return nlocal;
}
//@}


/// \name FOF routines related to modifying group ids
//@{

///This alters the group ids by an offset determined by the number of groups on all previous mpi threads so that the group
///has a unique group id. Prior to this, group ids are determined locally.
inline void MPIAdjustGroupIDs(const Int_t nbodies,Int_t *pfof) {
    Int_t noffset=0;
    for (int j=0;j<ThisTask;j++)noffset+=mpi_ngroups[j];
    for (Int_t i=0;i<nbodies;i++) if (pfof[i]>0) pfof[i]+=noffset;
}

///Collect FOF from all
void MPICollectFOF(const Int_t nbodies, Int_t *&pfof){
    Int_t sendTask,recvTask;
    MPI_Status status;
    //if using mpi, offset the pfof so that group ids are no unique, before just local to thread
    MPIAdjustGroupIDs(Nlocal,pfof);
    //now send the data from all MPI threads to thread zero
    //first must send how much data is local to a processor
    Int_t nsend_local[NProcs];
    for (int i=0;i<NProcs;i++) nsend_local[i]=0;
    if (ThisTask!=0)nsend_local[0]=Nlocal;
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    recvTask=0;
    //next copy task zero pfof into global mpi_pfof to the appropriate indices
    if (ThisTask==0) {
        for (Int_t i=0;i<Nlocal;i++) mpi_pfof[mpi_indexlist[i]]=pfof[i];
    }
    //then for each processor copy their values into local pfof and mpi_indexlist
    //note mpi_idlist contains id values whereas indexlist contains the index order of how particles were loaded
    //this requires determining the largest size needed
    if (ThisTask==0) {
        Int_t maxnlocal=0;
        for(int j=1;j<NProcs;j++) if (maxnlocal<mpi_nsend[j*NProcs]) maxnlocal=mpi_nsend[j*NProcs];
        delete[] pfof;
        delete[] mpi_indexlist;
        pfof=new Int_t[maxnlocal];
        mpi_indexlist=new Int_t[maxnlocal];
    }
    MPI_Barrier(MPI_COMM_WORLD);
    //now for each mpi task, copy appropriate data to mpi thread 0 local buffers
    for(int j=1;j<NProcs;j++)
    {
        sendTask=j;
        recvTask=0;
        if (ThisTask==sendTask) {
            MPI_Ssend(pfof, Nlocal , MPI_Int_t,0, TAG_FOF_D, MPI_COMM_WORLD);
            MPI_Ssend(mpi_indexlist, Nlocal , MPI_Int_t,0, TAG_FOF_E, MPI_COMM_WORLD);
        }
        if(ThisTask==0) {
            MPI_Recv(pfof, mpi_nsend[sendTask*NProcs], MPI_Int_t, sendTask, TAG_FOF_D, MPI_COMM_WORLD, &status);
            MPI_Recv(mpi_indexlist, mpi_nsend[sendTask*NProcs], MPI_Int_t, sendTask, TAG_FOF_E, MPI_COMM_WORLD, &status);
            for (Int_t i=0;i<mpi_nsend[sendTask*NProcs];i++) mpi_pfof[mpi_indexlist[i]]=pfof[i];
        }
        MPI_Barrier(MPI_COMM_WORLD);
    }
}
//@}

/// \name Routines related to distributing the grid cells used to calculate the coarse-grained mean field
//@{
/*! Collects all the grid data
*/
void MPIBuildGridData(const Int_t ngrid, GridCell *grid, Coordinate *gvel, Matrix *gveldisp){
    Int_t i, j,nthreads,nexport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Int_t sendTask,recvTask;
    MPI_Status status;

    for (j=0;j<NProcs;j++) nsend_local[j]=Ngridlocal;
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    noffset[0]=0;
    for (j=1;j<NProcs;j++) noffset[j]=noffset[j]+mpi_nsend[ThisTask+j*NProcs];
    for (i=0;i<Ngridlocal;i++) {
        for (j=0;j<3;j++) mpi_grid[noffset[ThisTask]+i].xm[j]=grid[i].xm[j];
        mpi_gvel[noffset[ThisTask]+i]=gvel[i];
        mpi_gveldisp[noffset[ThisTask]+i]=gveldisp[i];
    }

    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;//ThisTask^j;//bitwise XOR ensures that recvTask cycles around sendTask
            //blocking point-to-point send and receive.
            MPI_Sendrecv(grid,
                Ngridlocal* sizeof(struct GridCell), MPI_BYTE,
                recvTask, TAG_GRID_A,
                &mpi_grid[noffset[recvTask]],
                mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct GridCell),
                MPI_BYTE, recvTask, TAG_GRID_A, MPI_COMM_WORLD, &status);
            MPI_Sendrecv(gvel,
                Ngridlocal* sizeof(struct Coordinate), MPI_BYTE,
                recvTask, TAG_GRID_B,
                &mpi_gvel[noffset[recvTask]],
                mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct Coordinate),
                MPI_BYTE, recvTask, TAG_GRID_B, MPI_COMM_WORLD, &status);
            MPI_Sendrecv(gveldisp,
                Ngridlocal* sizeof(struct Matrix), MPI_BYTE,
                recvTask, TAG_GRID_C,
                &mpi_gveldisp[noffset[recvTask]],
                mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct Matrix),
                MPI_BYTE, recvTask, TAG_GRID_C, MPI_COMM_WORLD, &status);
        }
    }
}
//@}

/// \name comparison functions used to assign particles to a specific mpi thread
//@{
///comprasion function used to sort particles for export so that all particles being exported to the same processor are in a contiguous block and well ordered
///\todo, I should remove this and determine the ordering before hand.
int fof_export_cmp(const void *a, const void *b)
{
  if(((struct fofdata_in *) a)->Task < (((struct fofdata_in *) b)->Task))
    return -1;
  if(((struct fofdata_in *) a)->Task > (((struct fofdata_in *) b)->Task))
    return +1;
  return 0;
}

///comprasion function used to sort particles for export so that all particles being exported to the same processor are in a contiguous block and well ordered
int nn_export_cmp(const void *a, const void *b)
{
  if(((struct nndata_in *) a)->ToTask < (((struct nndata_in *) b)->ToTask))
    return -1;
  if(((struct nndata_in *) a)->ToTask > (((struct nndata_in *) b)->ToTask))
    return +1;
  return 0;
}

///comprasion function used to sort grouped particles so that easy to determine total number of groups locally, size of groups, etc.
int fof_id_cmp(const void *a, const void *b)
{
  if(((struct fofid_in *) a)->iGroup > (((struct fofid_in *) b)->iGroup))
    return -1;

  if(((struct fofid_in *) a)->iGroup < (((struct fofid_in *) b)->iGroup))
    return +1;

  if(((struct fofid_in *) a)->p.GetType() < (((struct fofid_in *) b)->p.GetType()))
    return -1;

  if(((struct fofid_in *) a)->p.GetType() > (((struct fofid_in *) b)->p.GetType()))
    return +1;


  if(((struct fofid_in *) a)->p.GetID() < (((struct fofid_in *) b)->p.GetID()))
    return -1;

  if(((struct fofid_in *) a)->p.GetID() > (((struct fofid_in *) b)->p.GetID()))
    return +1;

  return 0;
}
//@}

///\name mesh MPI decomposition related functions
//@{
vector<int> MPIGetCellListInSearchUsingMesh(Options &opt, Double_t xsearch[3][2], bool ignorelocalcells)
{
    int ixstart,iystart,izstart,ixend,iyend,izend,index;
    vector<int> celllist;
    ixstart=floor(xsearch[0][0]*opt.icellwidth[0]);
    ixend=floor(xsearch[0][1]*opt.icellwidth[0]);
    iystart=floor(xsearch[1][0]*opt.icellwidth[1]);
    iyend=floor(xsearch[1][1]*opt.icellwidth[1]);
    izstart=floor(xsearch[2][0]*opt.icellwidth[2]);
    izend=floor(xsearch[2][1]*opt.icellwidth[2]);

    for (auto ix=ixstart;ix<=ixend;ix++){
        for (auto iy=iystart;iy<=iyend;iy++){
            for (auto iz=izstart;iz<=izend;iz++){
                index=0;
                if (iz<0) index+=opt.numcellsperdim+iz;
                else if (iz>=opt.numcellsperdim) index+=iz-opt.numcellsperdim;
                else index+=iz;
                if (iy<0) index+=(opt.numcellsperdim+iy)*opt.numcellsperdim;
                else if (iy>=opt.numcellsperdim) index+=(iy-opt.numcellsperdim)*opt.numcellsperdim;
                else index+=iy*opt.numcellsperdim;
                if (ix<0) index+=(opt.numcellsperdim+ix)*opt.numcellsperdim*opt.numcellsperdim;
                else if (ix>=opt.numcellsperdim) index+=(ix-opt.numcellsperdim)*opt.numcellsperdim*opt.numcellsperdim;
                else index+=ix*opt.numcellsperdim*opt.numcellsperdim;
                if (ignorelocalcells && opt.cellnodeids[index]==ThisTask) continue;
                celllist.push_back(index);
            }
        }
    }
    return celllist;
}

//@}

#endif
